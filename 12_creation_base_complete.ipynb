{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3eb9649e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Fusion terminée. Fichier exporté : /Users/jeanjoelgoli/Documents/FINANCE/Travaux BRVM/Base_complète.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import re\n",
    "\n",
    "# --- chemins (à adapter si besoin) ---\n",
    "CSV1 = Path(\"combined_journalier_capi.csv\")   # ta 1ère table\n",
    "CSV2 = Path(\"Cours_BRVM.csv\")                 # ta 2ème table\n",
    "OUT  = Path(\"Base_complète.csv\")\n",
    "\n",
    "# --- lecture ---\n",
    "df1 = pd.read_csv(CSV1, encoding=\"utf-8-sig\")\n",
    "df2 = pd.read_csv(CSV2, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- nettoyage Ticker & Date ---\n",
    "def clean_ticker(s: pd.Series) -> pd.Series:\n",
    "    # supprime le suffixe \" J\" éventuel + espaces parasites\n",
    "    return (\n",
    "        s.astype(str)\n",
    "         .str.strip()\n",
    "         .str.replace(r\"\\s*J$\", \"\", regex=True)\n",
    "         .str.upper()\n",
    "    )\n",
    "\n",
    "def clean_date(s: pd.Series) -> pd.Series:\n",
    "    # normalise au format AAAA-MM-JJ\n",
    "    d = pd.to_datetime(s, errors=\"coerce\", dayfirst=False)\n",
    "    return d.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# 1) sur la table 1 : enlever \" J\" dans chaque ticker + normaliser Date\n",
    "if \"Ticker\" in df1.columns:\n",
    "    df1[\"Ticker\"] = clean_ticker(df1[\"Ticker\"])\n",
    "else:\n",
    "    raise ValueError(\"La 1ère table doit contenir une colonne 'Ticker'.\")\n",
    "\n",
    "if \"Date\" in df1.columns:\n",
    "    df1[\"Date\"] = clean_date(df1[\"Date\"])\n",
    "else:\n",
    "    raise ValueError(\"La 1ère table doit contenir une colonne 'Date'.\")\n",
    "\n",
    "# 2) sur la table 2 : normaliser Ticker + Date aussi\n",
    "if \"Ticker\" in df2.columns:\n",
    "    df2[\"Ticker\"] = clean_ticker(df2[\"Ticker\"])\n",
    "else:\n",
    "    raise ValueError(\"La 2ème table doit contenir une colonne 'Ticker'.\")\n",
    "\n",
    "if \"Date\" in df2.columns:\n",
    "    df2[\"Date\"] = clean_date(df2[\"Date\"])\n",
    "else:\n",
    "    raise ValueError(\"La 2ème table doit contenir une colonne 'Date'.\")\n",
    "\n",
    "# 3) retirer les colonnes à exclure côté table 2\n",
    "cols_a_exclure = {\n",
    "    \"Global capitalization\",\n",
    "    \"rang_capi\",\n",
    "    \"Number of shares\",\n",
    "    \"Global capitalization (%)\",\n",
    "}\n",
    "colonnes_df2 = [c for c in df2.columns if c not in cols_a_exclure]\n",
    "\n",
    "# on évite les doublons de colonnes Ticker/Date au merge\n",
    "colonnes_df2_sans_keys = [c for c in colonnes_df2 if c not in (\"Ticker\", \"Date\")]\n",
    "\n",
    "# 4) merge sur (Ticker, Date)\n",
    "# par défaut je fais un left join sur la table 1 (ta base principale)\n",
    "df_merged = df1.merge(\n",
    "    df2[[\"Ticker\", \"Date\"] + colonnes_df2_sans_keys],\n",
    "    on=[\"Ticker\", \"Date\"],\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"  # chaque (Ticker, Date) de df1 doit correspondre à 0/1 ligne de df2\n",
    ")\n",
    "\n",
    "# 5) optionnel : trier les colonnes (facultatif)\n",
    "# on garde l’ordre de df1 puis on ajoute ce qui vient de df2\n",
    "cols_final = list(df1.columns) + [c for c in colonnes_df2_sans_keys if c not in df1.columns]\n",
    "df_merged = df_merged.reindex(columns=cols_final)\n",
    "\n",
    "# 6) export\n",
    "df_merged.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
    "\n",
    "print(f\"✅ Fusion terminée. Fichier exporté : {OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "faad26c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/svq7n_mn2lg1fv5jbk66hzg00000gn/T/ipykernel_2667/1506569166.py:22: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  d = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
      "/var/folders/0n/svq7n_mn2lg1fv5jbk66hzg00000gn/T/ipykernel_2667/1506569166.py:22: UserWarning: Parsing dates in %d/%m/%Y format when dayfirst=False (the default) was specified. Pass `dayfirst=True` or specify a format to silence this warning.\n",
      "  d = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ 147859 lignes contiennent au moins un NA.\n",
      "Tickers avec NA : ['ABJC' 'BICB' 'BICC' 'BNBC' 'BOAB' 'BOABF' 'BOAC' 'BOAM' 'BOAN' 'BOAS'\n",
      " 'CABC' 'CBIBF' 'CFAC' 'CIEC' 'ECOC' 'ETIT' 'FTSC' 'LNBB' 'NEIC' 'NSBC'\n",
      " 'NTLC' 'ONTBF' 'ORAC' 'ORGT' 'PALC' 'PRSC' 'SAFC' 'SCRC' 'SDCC' 'SDSC'\n",
      " 'SEMC' 'SGBC' 'SHEC' 'SIBC' 'SICC' 'SIVC' 'SLBC' 'SMBC' 'SNTS' 'SOGC'\n",
      " 'SPHC' 'STAC' 'STBC' 'SVOC' 'TTLC' 'TTLS' 'UNLC' 'UNXC']\n",
      "\n",
      "✅ Export terminé : /Users/jeanjoelgoli/Documents/FINANCE/Travaux BRVM/Base_complète.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "# --- chemins d'entrée/sortie ---\n",
    "CSV1 = Path(\"combined_journalier_capi.csv\")\n",
    "CSV2 = Path(\"Cours_BRVM.csv\")\n",
    "CSV3 = Path(\"richbourse_societes.csv\")\n",
    "CSV4 = Path(\"dividende_histo.csv\")\n",
    "OUT  = Path(\"Base_complète.csv\")\n",
    "\n",
    "# --- helpers ---\n",
    "def clean_ticker_col(series: pd.Series) -> pd.Series:\n",
    "    return (\n",
    "        series.astype(str)\n",
    "              .str.strip()\n",
    "              .str.replace(r\"\\s*J$\", \"\", regex=True)\n",
    "              .str.upper()\n",
    "    )\n",
    "\n",
    "def clean_date_col(series: pd.Series) -> pd.Series:\n",
    "    # Si tes CSV sont au format français, mets dayfirst=True\n",
    "    d = pd.to_datetime(series, errors=\"coerce\", dayfirst=False)\n",
    "    return d.dt.strftime(\"%Y-%m-%d\")\n",
    "\n",
    "# --- lecture ---\n",
    "df1 = pd.read_csv(CSV1, encoding=\"utf-8-sig\")\n",
    "df2 = pd.read_csv(CSV2, encoding=\"utf-8-sig\")\n",
    "df3 = pd.read_csv(CSV3, encoding=\"utf-8-sig\")\n",
    "df4 = pd.read_csv(CSV4, encoding=\"utf-8-sig\")\n",
    "\n",
    "# --- nettoyage table 1 (gauche) ---\n",
    "df1[\"Ticker\"] = clean_ticker_col(df1[\"Ticker\"])\n",
    "df1[\"Date\"]   = clean_date_col(df1[\"Date\"])\n",
    "\n",
    "# --- nettoyage table 2 ---\n",
    "df2[\"Ticker\"] = clean_ticker_col(df2[\"Ticker\"])\n",
    "df2[\"Date\"]   = clean_date_col(df2[\"Date\"])\n",
    "\n",
    "cols_exclure = {\n",
    "    \"Global capitalization\",\n",
    "    \"rang_capi\",\n",
    "    \"Number of shares\",\n",
    "    \"Global capitalization (%)\",\n",
    "}\n",
    "cols_df2_keep = [c for c in df2.columns if c not in cols_exclure]\n",
    "cols_df2_payload = [c for c in cols_df2_keep if c not in (\"Ticker\", \"Date\")]\n",
    "\n",
    "# rendre unique sur (Ticker, Date)\n",
    "df2_right = df2[[\"Ticker\", \"Date\"] + cols_df2_payload].sort_values([\"Ticker\", \"Date\"])\n",
    "df2_right = df2_right.drop_duplicates([\"Ticker\", \"Date\"], keep=\"last\")\n",
    "\n",
    "df_merged = df1.merge(\n",
    "    df2_right,\n",
    "    on=[\"Ticker\", \"Date\"],\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "# --- nettoyage table 3 ---\n",
    "df3 = df3.rename(columns={c: \"Ticker\" for c in df3.columns if c.lower() == \"ticker\"})\n",
    "df3[\"Ticker\"] = clean_ticker_col(df3[\"Ticker\"])\n",
    "cols_df3_payload = [c for c in df3.columns if c != \"Ticker\"]\n",
    "\n",
    "# rendre unique sur (Ticker)\n",
    "df3_right = df3[[\"Ticker\"] + cols_df3_payload].sort_values([\"Ticker\"])\n",
    "df3_right = df3_right.drop_duplicates([\"Ticker\"], keep=\"last\")\n",
    "\n",
    "df_merged = df_merged.merge(\n",
    "    df3_right,\n",
    "    on=\"Ticker\",\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "# --- nettoyage table 4 (dividendes) ---\n",
    "df4 = df4.rename(columns={c: \"Ticker\" for c in df4.columns if c.lower() == \"ticker\"})\n",
    "df4[\"Ticker\"] = clean_ticker_col(df4[\"Ticker\"])\n",
    "df4[\"Ex-dividende\"]  = clean_date_col(df4[\"Ex-dividende\"])\n",
    "df4[\"Date paiement\"] = clean_date_col(df4[\"Date paiement\"])\n",
    "\n",
    "cols_df4_payload = [c for c in df4.columns if c not in (\"Ticker\", \"Ex-dividende\")]\n",
    "\n",
    "# rendre unique sur (Ticker, Ex-dividende) puis renommer en Date\n",
    "df4_tmp = df4.rename(columns={\"Ex-dividende\": \"Date\"})\n",
    "df4_right = df4_tmp[[\"Ticker\", \"Date\"] + cols_df4_payload].sort_values([\"Ticker\", \"Date\"])\n",
    "df4_right = df4_right.drop_duplicates([\"Ticker\", \"Date\"], keep=\"last\")\n",
    "\n",
    "df_merged = df_merged.merge(\n",
    "    df4_right,\n",
    "    on=[\"Ticker\", \"Date\"],\n",
    "    how=\"left\",\n",
    "    validate=\"m:1\"\n",
    ")\n",
    "\n",
    "# --- vérification NA ---\n",
    "na_mask = df_merged.isna().any(axis=1)\n",
    "nb_lignes_na = na_mask.sum()\n",
    "if nb_lignes_na == 0:\n",
    "    print(\"✅ Aucune valeur manquante (NA) dans la table finale.\")\n",
    "else:\n",
    "    print(f\"⚠️ {nb_lignes_na} lignes contiennent au moins un NA.\")\n",
    "    tickers_na = df_merged.loc[na_mask, \"Ticker\"].dropna().unique()\n",
    "    print(\"Tickers avec NA :\", tickers_na)\n",
    "\n",
    "# --- export ---\n",
    "df_merged.to_csv(OUT, index=False, encoding=\"utf-8-sig\")\n",
    "print(f\"\\n✅ Export terminé : {OUT.resolve()}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6070c595",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/svq7n_mn2lg1fv5jbk66hzg00000gn/T/ipykernel_2667/3755240380.py:10: DtypeWarning: Columns (32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(SRC)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✔ Fichier exporté : Base_complète_div.csv (147859 lignes)\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "SRC = \"Base_complète.csv\"\n",
    "DST = \"Base_complète_div.csv\"\n",
    "\n",
    "# Charge le fichier\n",
    "df = pd.read_csv(SRC)\n",
    "\n",
    "# Assure-toi que les colonnes attendues existent\n",
    "needed = [\"Ticker\",\"Date\",\"Cours Ajuste\",\"Dividende ajusté\"]\n",
    "missing = [c for c in needed if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes: {missing}\")\n",
    "\n",
    "# Convertit en float\n",
    "df[\"Cours Ajuste\"] = pd.to_numeric(df[\"Cours Ajuste\"], errors=\"coerce\")\n",
    "df[\"Dividende ajusté\"] = pd.to_numeric(df[\"Dividende ajusté\"], errors=\"coerce\").fillna(0.0)\n",
    "\n",
    "# --- 1) Cours Ajuste AddDiv ---\n",
    "df[\"Cours Ajuste AddDiv\"] = df[\"Cours Ajuste\"] + df[\"Dividende ajusté\"]\n",
    "\n",
    "# --- 2) Cours Ajuste BackcorrectDiv ---\n",
    "# Idée : pour chaque Ticker, on calcule un facteur cumulatif rétroactif\n",
    "dfs = []\n",
    "for t, dft in df.groupby(\"Ticker\"):\n",
    "    dft = dft.sort_values(\"Date\")  # ordonne chronologiquement\n",
    "    dft[\"PrevCours\"] = dft[\"Cours Ajuste\"].shift(1)\n",
    "\n",
    "    # ratio = (P_{t-1} - Div) / P_{t-1} aux dates ex-div\n",
    "    dft[\"ratio\"] = 1.0\n",
    "    mask = dft[\"Dividende ajusté\"] > 0\n",
    "    dft.loc[mask, \"ratio\"] = (dft.loc[mask, \"PrevCours\"] - dft.loc[mask, \"Dividende ajusté\"]) / dft.loc[mask, \"PrevCours\"]\n",
    "\n",
    "    # cumprod rétroactif (du présent vers le passé)\n",
    "    adj_factor = dft[\"ratio\"][::-1].cumprod()[::-1]\n",
    "    dft[\"Cours Ajuste BackcorrectDiv\"] = dft[\"Cours Ajuste\"] * adj_factor\n",
    "\n",
    "    dfs.append(dft.drop(columns=[\"PrevCours\",\"ratio\"]))\n",
    "\n",
    "df_out = pd.concat(dfs, ignore_index=True)\n",
    "\n",
    "# Sauvegarde\n",
    "df_out.to_csv(DST, index=False, encoding=\"utf-8\")\n",
    "print(f\"✔ Fichier exporté : {DST} ({len(df_out)} lignes)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
