{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9626d4f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ABJC ...\n",
      "üìä BICB ...\n",
      "üìä BICC ...\n",
      "üìä BNBC ...\n",
      "üìä BOAB ...\n",
      "üìä BOABF ...\n",
      "üìä BOAC ...\n",
      "üìä BOAM ...\n",
      "üìä BOAN ...\n",
      "üìä BOAS ...\n",
      "üìä CABC ...\n",
      "üìä CBIBF ...\n",
      "üìä CFAC ...\n",
      "üìä CIEC ...\n",
      "üìä ECOC ...\n",
      "üìä ETIT ...\n",
      "üìä FTSC ...\n",
      "üìä LNBB ...\n",
      "üìä NEIC ...\n",
      "üìä NSBC ...\n",
      "üìä NTLC ...\n",
      "üìä ONTBF ...\n",
      "üìä ORAC ...\n",
      "üìä ORGT ...\n",
      "üìä PALC ...\n",
      "üìä PRSC ...\n",
      "üìä SAFC ...\n",
      "üìä SCRC ...\n",
      "üìä SDCC ...\n",
      "üìä SDSC ...\n",
      "üìä SEMC ...\n",
      "üìä SGBC ...\n",
      "üìä SHEC ...\n",
      "üìä SIBC ...\n",
      "üìä SICC ...\n",
      "üìä SIVC ...\n",
      "üìä SLBC ...\n",
      "üìä SMBC ...\n",
      "üìä SNTS ...\n",
      "üìä SOGC ...\n",
      "üìä SPHC ...\n",
      "üìä STAC ...\n",
      "üìä STBC ...\n",
      "üìä SVOC ...\n",
      "üìä TTLC ...\n",
      "üìä TTLS ...\n",
      "üìä UNLC ...\n",
      "üìä UNXC ...\n",
      "‚úÖ Fichier rapport_brvm_complet.csv g√©n√©r√© (1585 lignes)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# --------- Param√®tres ---------\n",
    "TICKERS = [\n",
    "    \"ABJC\",\"BICB\",\"BICC\",\"BNBC\",\"BOAB\",\"BOABF\",\"BOAC\",\"BOAM\",\"BOAN\",\"BOAS\",\n",
    "    \"CABC\",\"CBIBF\",\"CFAC\",\"CIEC\",\"ECOC\",\"ETIT\",\"FTSC\",\"LNBB\",\"NEIC\",\"NSBC\",\n",
    "    \"NTLC\",\"ONTBF\",\"ORAC\",\"ORGT\",\"PALC\",\"PRSC\",\"SAFC\",\"SCRC\",\"SDCC\",\"SDSC\",\n",
    "    \"SEMC\",\"SGBC\",\"SHEC\",\"SIBC\",\"SICC\",\"SIVC\",\"SLBC\",\"SMBC\",\"SNTS\",\"SOGC\",\n",
    "    \"SPHC\",\"STAC\",\"STBC\",\"SVOC\",\"TTLC\",\"TTLS\",\"UNLC\",\"UNXC\"\n",
    "]\n",
    "\n",
    "BASE_URL   = \"https://www.richbourse.com/investisseur/rapport-activite/index/{ticker}/{type_}/{periode}\"\n",
    "AUTRES_URL = \"https://www.richbourse.com/investisseur/analyse-societe/rapports-autres/{ticker}\"\n",
    "\n",
    "# --------- Helpers ---------\n",
    "def clean_cell(val: str) -> str:\n",
    "    if not val:\n",
    "        return \"\"\n",
    "    # coupe la variation (%) et nettoie espaces/virgules\n",
    "    val = re.split(r\"\\(\", val)[0]\n",
    "    val = (val.replace(\"\\xa0\", \" \")\n",
    "              .replace(\" \", \"\")\n",
    "              .replace(\",\", \"\"))\n",
    "    return val.strip()\n",
    "\n",
    "def detect_unit_footer(soup: BeautifulSoup) -> str:\n",
    "    patterns = [\n",
    "        r\"donn[√©e]es?.*xof\",\n",
    "        r\"donn[√©e]es?.*francs?\\s*cfa\",\n",
    "        r\"donn[√©e]es?.*milliers.*xof\",\n",
    "        r\"en\\s+milliers\\s+de\\s+xof\"\n",
    "    ]\n",
    "    # on balaie pas mal d'√©l√©ments de fin de page\n",
    "    for el in soup.select(\"p, small, div, span, footer\")[::-1]:\n",
    "        txt = el.get_text(\" \", strip=True)\n",
    "        if not txt:\n",
    "            continue\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, txt, flags=re.I):\n",
    "                return txt\n",
    "    return \"\"\n",
    "\n",
    "def period_label_from_type(type_: str, periode: str) -> str:\n",
    "    t = type_.lower()\n",
    "    if t.startswith(\"trimes\"):\n",
    "        return f\"T{periode}\"\n",
    "    if t.startswith(\"semest\"):\n",
    "        return \"S1\" if periode == \"1\" else \"S2\"\n",
    "    return \"An\"\n",
    "\n",
    "def period_label_from_text(txt: str) -> str:\n",
    "    \"\"\"Devine S1/S2/T1..T4/An √† partir d'un libell√© libre (rapports-autres).\"\"\"\n",
    "    s = txt.lower()\n",
    "    # trimestre\n",
    "    m = re.search(r\"(\\bt\\s*([1-4])\\b|\\btrimes?tre\\s*([1-4])\\b|\\b([1-4])\\s*e?r?\\s*trimes?tre\\b)\", s)\n",
    "    if m:\n",
    "        # capture du num√©ro\n",
    "        for g in m.groups()[1:]:\n",
    "            if g and g.isdigit():\n",
    "                return f\"T{g}\"\n",
    "    # semestre\n",
    "    m = re.search(r\"semestre\\s*([12])|\\b([12])\\s*e?r?\\s*semestre\\b|\\bs([12])\\b\", s)\n",
    "    if m:\n",
    "        for g in m.groups():\n",
    "            if g and g.isdigit():\n",
    "                return f\"S{g}\"\n",
    "    return \"An\"\n",
    "\n",
    "def firstcol_contains_year(text: str, year: int) -> bool:\n",
    "    return str(year) in (text or \"\")\n",
    "\n",
    "def extract_headers_and_rows(table) -> tuple[list[str], list[list[str]]]:\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [th.get_text(\" \", strip=True) for th in thead.find_all(\"th\")]\n",
    "    else:\n",
    "        first_tr = table.find(\"tr\")\n",
    "        headers = [td.get_text(\" \", strip=True) for td in first_tr.find_all([\"td\",\"th\"])]\n",
    "    rows = []\n",
    "    for tr in table.find_all(\"tr\"):\n",
    "        tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "        if tds:\n",
    "            rows.append(tds)\n",
    "    return headers, rows\n",
    "\n",
    "# --------- Scrapers ---------\n",
    "async def scrape_report(page, ticker: str, type_: str, periode: str) -> pd.DataFrame:\n",
    "    \"\"\"Annuel/Semestriel/Trimestriel (tableau principal).\"\"\"\n",
    "    url = BASE_URL.format(ticker=ticker, type_=type_, periode=periode)\n",
    "    await page.goto(url, timeout=60000)\n",
    "\n",
    "    try:\n",
    "        await page.wait_for_selector(\"table.table\", timeout=8000)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    html_page = await page.content()\n",
    "    soup_page = BeautifulSoup(html_page, \"html.parser\")\n",
    "    table = soup_page.select_one(\"table.table\")\n",
    "    if table is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # ent√™tes\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "    else:\n",
    "        first_tr = table.find(\"tbody\").find(\"tr\")\n",
    "        headers = [td.get_text(strip=True) for td in first_tr.find_all(\"td\")]\n",
    "\n",
    "    if \"P√©riode\" not in headers:\n",
    "        headers = [\"P√©riode\"] + headers\n",
    "\n",
    "    # lignes\n",
    "    rows = []\n",
    "    body = table.find(\"tbody\")\n",
    "    if body:\n",
    "        for tr in body.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds:\n",
    "                continue\n",
    "            if len(tds) == len(headers) - 1 and headers[0] == \"P√©riode\":\n",
    "                tds = [\"\"] + tds  # s√©cu si la p√©riode n‚Äôest pas rendue\n",
    "            row = {headers[i]: clean_cell(tds[i]) for i in range(min(len(headers), len(tds)))}\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # unit√©\n",
    "    unit_phrase = detect_unit_footer(soup_page)\n",
    "\n",
    "    # colonnes additionnelles\n",
    "    type_norm = \"Annuel\" if type_.lower().startswith(\"annuel\") else (\"Semestriel\" if type_.lower().startswith(\"semest\") else \"Trimestriel\")\n",
    "    df[\"Ticker\"] = ticker\n",
    "    df[\"Type\"] = type_norm\n",
    "    df[\"P√©riode_d√©tail\"] = period_label_from_type(type_norm, periode)\n",
    "    df[\"en XOF\"] = unit_phrase\n",
    "\n",
    "    # ordre\n",
    "    front = [\"Ticker\", \"Type\", \"P√©riode_d√©tail\", \"P√©riode\", \"en XOF\"]\n",
    "    others = [c for c in df.columns if c not in front]\n",
    "    return df[front + others]\n",
    "\n",
    "async def scrape_rapports_autres(page, ticker: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    /rapports-autres/{ticker}\n",
    "    - Conserve uniquement les lignes dont la 1 ≥·µâ cellule contient l'ann√©e courante\n",
    "    - Devine P√©riode_d√©tail (S1/S2/T1..T4/An) depuis le libell√© de la 1 ≥·µâ cellule\n",
    "    - P√©riode = ann√©e d√©tect√©e (courante)\n",
    "    - en XOF = note sous le 1er tableau (fallback: d√©tection globale)\n",
    "    \"\"\"\n",
    "    url = AUTRES_URL.format(ticker=ticker)\n",
    "    await page.goto(url, timeout=60000)\n",
    "\n",
    "    try:\n",
    "        await page.wait_for_selector(\"table\", timeout=8000)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    html = await page.content()\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    tables = soup.select(\"table\")\n",
    "    if not tables:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    current_year = datetime.now().year\n",
    "\n",
    "    # Unit√© : d‚Äôabord l‚Äô√©l√©ment suivant le 1er tableau, sinon fallback global\n",
    "    unit_phrase = \"\"\n",
    "    first_table = tables[0]\n",
    "    sib = first_table.find_next_sibling()\n",
    "    while sib and sib.name in [\"table\",\"script\",\"style\"]:\n",
    "        sib = sib.find_next_sibling()\n",
    "    if sib:\n",
    "        txt = sib.get_text(\" \", strip=True)\n",
    "        if re.search(r\"(donn[√©e]es?|en)\\s+.*(xof|francs?\\s*cfa|milliers)\", txt, flags=re.I):\n",
    "            unit_phrase = txt\n",
    "    if not unit_phrase:\n",
    "        unit_phrase = detect_unit_footer(soup)\n",
    "\n",
    "    collected = []\n",
    "    for table in tables:\n",
    "        headers, rows = extract_headers_and_rows(table)\n",
    "        if not headers or not rows:\n",
    "            continue\n",
    "\n",
    "        if \"P√©riode\" not in headers:\n",
    "            headers = [\"P√©riode\"] + headers\n",
    "\n",
    "        for r in rows:\n",
    "            # force une colonne \"P√©riode\" si absente\n",
    "            if len(r) == len(headers) - 1:\n",
    "                r = [\"\"] + r\n",
    "            if not r:\n",
    "                continue\n",
    "            first_cell_raw = r[0]\n",
    "            if not firstcol_contains_year(first_cell_raw, current_year):\n",
    "                continue\n",
    "\n",
    "            row_dict = {headers[i]: clean_cell(r[i]) for i in range(min(len(headers), len(r)))}\n",
    "            # Ajouts format final\n",
    "            row_dict[\"Ticker\"] = ticker\n",
    "            row_dict[\"Type\"] = \"Autres\"\n",
    "            row_dict[\"P√©riode_d√©tail\"] = period_label_from_text(first_cell_raw)\n",
    "            row_dict[\"P√©riode\"] = str(current_year)\n",
    "            row_dict[\"en XOF\"] = unit_phrase\n",
    "            collected.append(row_dict)\n",
    "\n",
    "    if not collected:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    df = pd.DataFrame(collected)\n",
    "    front = [\"Ticker\", \"Type\", \"P√©riode_d√©tail\", \"P√©riode\", \"en XOF\"]\n",
    "    others = [c for c in df.columns if c not in front]\n",
    "    return df[front + others]\n",
    "\n",
    "# --------- Orchestrateur ---------\n",
    "async def main(output_csv: str = \"rapport_brvm_complet.csv\", headless: bool = True):\n",
    "    all_parts = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless)\n",
    "        try:\n",
    "            context = await browser.new_context(storage_state=\"cookies.json\")\n",
    "        except:\n",
    "            context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for ticker in TICKERS:\n",
    "            print(f\"üìä {ticker} ...\")\n",
    "            dfs = []\n",
    "            # Annuel\n",
    "            dfs.append(await scrape_report(page, ticker, \"Annuel\", \"1\"))\n",
    "            # Semestriels\n",
    "            dfs.append(await scrape_report(page, ticker, \"Semestriel\", \"1\"))\n",
    "            dfs.append(await scrape_report(page, ticker, \"Semestriel\", \"2\"))\n",
    "            # Trimestriels\n",
    "            for i in range(1, 4+1):\n",
    "                dfs.append(await scrape_report(page, ticker, \"Trimestriel\", str(i)))\n",
    "            # Rapports \"autres\" (filtr√©s sur l'ann√©e courante)\n",
    "            dfs.append(await scrape_rapports_autres(page, ticker))\n",
    "\n",
    "            part = pd.concat([d for d in dfs if isinstance(d, pd.DataFrame) and not d.empty],\n",
    "                             ignore_index=True) if dfs else pd.DataFrame()\n",
    "\n",
    "            if part.empty:\n",
    "                all_parts.append(pd.DataFrame([{\n",
    "                    \"Ticker\": ticker, \"Type\": \"Aucune donn√©e\",\n",
    "                    \"P√©riode_d√©tail\": \"\", \"P√©riode\": \"\", \"en XOF\": \"\"\n",
    "                }]))\n",
    "            else:\n",
    "                all_parts.append(part)\n",
    "\n",
    "            time.sleep(0.5 + random.random()*0.7)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df_final = pd.concat(all_parts, ignore_index=True) if all_parts else pd.DataFrame()\n",
    "    if df_final.empty:\n",
    "        print(\"‚ö†Ô∏è Aucun rapport r√©cup√©r√©\")\n",
    "        return df_final\n",
    "\n",
    "    first_cols = [\"Ticker\", \"Type\", \"P√©riode_d√©tail\", \"P√©riode\", \"en XOF\"]\n",
    "    df_final = df_final[first_cols + [c for c in df_final.columns if c not in first_cols]]\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úÖ Fichier {output_csv} g√©n√©r√© ({len(df_final)} lignes)\")\n",
    "    return df_final\n",
    "\n",
    "# --- Ex√©cution dans Jupyter ---\n",
    "df = await main(headless=False)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "46b75a0b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé Groupes en doublon sur ('Ticker', 'P√©riode_d√©tail', 'P√©riode') : 49\n",
      "‚úÖ Doublons (Ticker, P√©riode_d√©tail, P√©riode) r√©solus en gardant la ligne la plus compl√®te.\n",
      "‚úÖ Export r√©alis√© dans rapport_brvm_complet_uniformise.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "# Charger le fichier\n",
    "df = pd.read_csv(\"rapport_brvm_complet.csv\")\n",
    "\n",
    "# --- Uniformisation en XOF (reprend la logique pr√©c√©dente, en vectoris√©) ---\n",
    "def get_multiplier(unit: str) -> int:\n",
    "    if pd.isna(unit):\n",
    "        return 1\n",
    "    unit = str(unit).lower()\n",
    "    if \"milliers\" in unit:\n",
    "        return 1_000\n",
    "    elif \"millions\" in unit:\n",
    "        return 1_000_000\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "# Colonnes num√©riques √† corriger (√† partir de la 6e jusqu'√† la fin)\n",
    "num_cols = df.columns[5:]\n",
    "\n",
    "# Calcul vectoris√© du facteur\n",
    "factors = df[\"en XOF\"].map(get_multiplier)\n",
    "\n",
    "# Conversion s√ªre et multiplication\n",
    "df[num_cols] = df[num_cols].apply(\n",
    "    lambda s: pd.to_numeric(s.replace(\"-\", np.nan), errors=\"coerce\") * factors\n",
    ")\n",
    "\n",
    "# Unit√©s uniformis√©es\n",
    "df[\"en XOF\"] = \"XOF\"\n",
    "\n",
    "# --- Ops demand√©es ---\n",
    "# 1) Supprimer la colonne \"Type\" si elle existe\n",
    "if \"Type\" in df.columns:\n",
    "    df = df.drop(columns=[\"Type\"])\n",
    "\n",
    "# 2) \"P√©riode\" -> 4 derniers caract√®res (ann√©e)\n",
    "if \"P√©riode\" in df.columns:\n",
    "    annee = (\n",
    "        df[\"P√©riode\"].astype(str).str[-4:].str.extract(r\"(\\d{4})\")[0].astype(\"Int64\")\n",
    "    )\n",
    "    df[\"P√©riode\"] = annee\n",
    "\n",
    "# --- D√©duplication avanc√©e sur (Ticker, P√©riode_d√©tail, P√©riode) en gardant la ligne la plus compl√®te ---\n",
    "subset = [\"Ticker\", \"P√©riode_d√©tail\", \"P√©riode\"]\n",
    "missing_subset = [c for c in subset if c not in df.columns]\n",
    "if missing_subset:\n",
    "    raise KeyError(f\"Colonnes manquantes pour la d√©duplication: {missing_subset}\")\n",
    "\n",
    "# Compter \"l'information\" pr√©sente par ligne (champ non vide, non '-', non NaN)\n",
    "# Sans modifier df: on cr√©e une copie normalis√©e pour le scoring\n",
    "tmp = df.replace(\n",
    "    to_replace=[r\"^\\s*$\", r\"^-+$\", r\"(?i)^nan$\"],\n",
    "    value=pd.NA,\n",
    "    regex=True\n",
    ")\n",
    "\n",
    "# Score global de compl√©tude\n",
    "score_all = tmp.notna().sum(axis=1)\n",
    "\n",
    "# Score num√©rique (optionnel, utile comme briseur d'√©galit√©)\n",
    "numeric_cols = [c for c in df.columns if c in num_cols]\n",
    "score_num = tmp[numeric_cols].notna().sum(axis=1) if numeric_cols else pd.Series(0, index=df.index)\n",
    "\n",
    "# Cr√©er une cl√© de tri pour choisir la \"meilleure\" ligne dans chaque groupe\n",
    "df[\"__score_all__\"] = score_all\n",
    "df[\"__score_num__\"] = score_num\n",
    "\n",
    "# Identifier les doublons (groupes ayant au moins 2 lignes)\n",
    "dupe_groups = df.duplicated(subset=subset, keep=False)\n",
    "nb_groups = (\n",
    "    df.loc[dupe_groups]\n",
    "      .drop_duplicates(subset=subset)\n",
    "      .shape[0]\n",
    ")\n",
    "\n",
    "print(f\"üîé Groupes en doublon sur {tuple(subset)} : {nb_groups}\")\n",
    "\n",
    "# Garder, pour chaque groupe, l'index de la ligne avec le score max (puis score_num en tie-break)\n",
    "# On tri d‚Äôabord par scores desc, puis on prend le premier de chaque groupe\n",
    "df_sorted = df.sort_values(by=[\"__score_all__\", \"__score_num__\"], ascending=False)\n",
    "best_idx = df_sorted.groupby(subset, dropna=False, as_index=False).head(1).index\n",
    "\n",
    "# Conserver ces lignes \"meilleures\" + toutes les lignes uniques (non en doublon)\n",
    "df_dedup = pd.concat([\n",
    "    df.loc[~dupe_groups],     # uniques\n",
    "    df.loc[best_idx]          # meilleurs des groupes\n",
    "], ignore_index=True)\n",
    "\n",
    "# Supprimer d‚Äô√©ventuels doublons r√©siduels exacts et nettoyer colonnes techniques\n",
    "df_dedup = df_dedup.drop(columns=[\"__score_all__\", \"__score_num__\"], errors=\"ignore\")\n",
    "df_dedup = df_dedup.drop_duplicates().reset_index(drop=True)\n",
    "\n",
    "# Export\n",
    "df_dedup.to_csv(\"rapport_brvm_complet_uniformise.csv\", index=False)\n",
    "# Export en Excel\n",
    "df_dedup.to_excel(\"rapport_brvm_complet_uniformise.xlsx\", index=False)\n",
    "\n",
    "print(\"‚úÖ Doublons (Ticker, P√©riode_d√©tail, P√©riode) r√©solus en gardant la ligne la plus compl√®te.\")\n",
    "print(\"‚úÖ Export r√©alis√© dans rapport_brvm_complet_uniformise.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1dfd229e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fichier pr√™t : rapport_brvm_complet_uniformise2.csv\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Charger le CSV\n",
    "df = pd.read_csv(\"rapport_brvm_complet_uniformise.csv\")\n",
    "\n",
    "# Colonnes utiles\n",
    "colonnes_utiles = [\n",
    "    \"Ticker\", \"P√©riode_d√©tail\", \"P√©riode\",\n",
    "    \"Chiffre d'Affaires\", \"Produit net bancaire\",\n",
    "    \"Total Bilan\",\n",
    "    \"R√©sultat d'Exploitation\", \"R√©sultat Brut d'Exploitation\",\n",
    "    \"R√©sultat Net\"\n",
    "]\n",
    "df = df[colonnes_utiles]\n",
    "\n",
    "# --- Liste compl√®te des banques cot√©es √† la BRVM ---\n",
    "banques = [\n",
    "    \"BICB\",\"BICC\",\"BOAB\",\"BOABF\",\"BOAC\",\"BOAM\",\"BOAN\",\"BOAS\",\n",
    "    \"CBIBF\",\"ECOC\",\"NSBC\",\"SGBC\",\"SIBC\",\"ETIT\"  # <-- inclut ETIT\n",
    "]\n",
    "\n",
    "# --- Colonnes calcul√©es ---\n",
    "df[\"CA_PNB\"] = np.where(\n",
    "    df[\"Ticker\"].isin(banques),\n",
    "    df[\"Produit net bancaire\"],\n",
    "    df[\"Chiffre d'Affaires\"]\n",
    ")\n",
    "\n",
    "df[\"RE_RBE\"] = np.where(\n",
    "    df[\"Ticker\"].isin(banques),\n",
    "    df[\"R√©sultat Brut d'Exploitation\"],\n",
    "    df[\"R√©sultat d'Exploitation\"]\n",
    ")\n",
    "\n",
    "# Ajouter une colonne Capitaux propres vide\n",
    "df[\"Capitaux propres\"] = None\n",
    "\n",
    "# Colonnes finales\n",
    "df_final = df[[\n",
    "    \"Ticker\", \"P√©riode_d√©tail\", \"P√©riode\",\n",
    "    \"CA_PNB\", \"Total Bilan\", \"RE_RBE\", \"R√©sultat Net\", \"Capitaux propres\"\n",
    "]]\n",
    "\n",
    "# Export\n",
    "df_final.to_csv(\"rapport_brvm_complet_uniformise2.csv\", index=False)\n",
    "\n",
    "print(\"‚úÖ Fichier pr√™t : rapport_brvm_complet_uniformise2.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b5790980",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Pivot compl√©t√© & tri√© (T‚ÜíS‚ÜíAn) et colonnes group√©es par ticker export√© : rapport_brvm_complet_uniformise3.csv\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from itertools import product\n",
    "\n",
    "# Fichiers\n",
    "SRC = \"rapport_brvm_complet_uniformise2.csv\"\n",
    "DST = \"rapport_brvm_complet_uniformise3.csv\"\n",
    "\n",
    "# Variables √† pivoter\n",
    "vars_keep = [\"CA_PNB\", \"Total Bilan\", \"RE_RBE\", \"R√©sultat Net\", \"Capitaux propres\"]\n",
    "\n",
    "# Charge\n",
    "df = pd.read_csv(SRC)\n",
    "\n",
    "# V√©rifications\n",
    "cols_needed = [\"Ticker\",\"P√©riode_d√©tail\",\"P√©riode\"] + vars_keep\n",
    "missing = [c for c in cols_needed if c not in df.columns]\n",
    "if missing:\n",
    "    raise ValueError(f\"Colonnes manquantes dans {SRC}: {missing}\")\n",
    "\n",
    "# D√©dupe\n",
    "df = df.sort_values([\"P√©riode_d√©tail\",\"P√©riode\",\"Ticker\"]).drop_duplicates(\n",
    "    subset=[\"P√©riode_d√©tail\",\"P√©riode\",\"Ticker\"], keep=\"first\"\n",
    ")\n",
    "\n",
    "# Normalise \"P√©riode\" -> ann√©e num√©rique\n",
    "def extract_year(x):\n",
    "    s = str(x)\n",
    "    m = re.search(r\"\\d{4}\", s)\n",
    "    return int(m.group(0)) if m else np.nan\n",
    "\n",
    "df[\"__Year\"] = df[\"P√©riode\"].apply(extract_year)\n",
    "\n",
    "# Range d'ann√©es: 1998 -> max ann√©e d√©tect√©e (au moins 1998)\n",
    "year_min = 1998\n",
    "year_max = int(np.nanmax(df[\"__Year\"])) if df[\"__Year\"].notna().any() else 1998\n",
    "years = list(range(year_min, year_max + 1))\n",
    "\n",
    "# Nouvel ordre souhait√©: T1..T4, S1..S2, puis An\n",
    "ordre_pd = [\"T1\",\"T2\",\"T3\",\"T4\",\"S1\",\"S2\",\"An\"]\n",
    "df[\"__PeriodeOrd\"] = pd.Categorical(df[\"P√©riode_d√©tail\"], categories=ordre_pd, ordered=True)\n",
    "\n",
    "# Tickers\n",
    "tickers = sorted(df[\"Ticker\"].unique())\n",
    "\n",
    "# Index complet (P√©riode_d√©tail, Year, Ticker)\n",
    "full_index = pd.MultiIndex.from_tuples(\n",
    "    list(product(ordre_pd, years, tickers)),\n",
    "    names=[\"P√©riode_d√©tail\", \"__Year\", \"Ticker\"]\n",
    ")\n",
    "\n",
    "# R√©indexe pour compl√©ter les manquants\n",
    "base = df.set_index([\"P√©riode_d√©tail\",\"__Year\",\"Ticker\"])\n",
    "base = base[vars_keep].reindex(full_index)\n",
    "\n",
    "# Reconstruit \"P√©riode\" depuis l'ann√©e\n",
    "base = base.reset_index()\n",
    "base[\"P√©riode\"] = base[\"__Year\"].astype(\"Int64\").astype(str)\n",
    "\n",
    "# Pivot large: colonnes \"Ticker_Variable\"\n",
    "base = base.set_index([\"P√©riode_d√©tail\",\"P√©riode\",\"Ticker\"])\n",
    "pieces = []\n",
    "for v in vars_keep:\n",
    "    w = base[v].unstack(\"Ticker\")      # colonnes=Tickers\n",
    "    w = w.add_suffix(f\"_{v}\")          # \"ABJC_CA_PNB\", etc.\n",
    "    pieces.append(w)\n",
    "\n",
    "wide = pd.concat(pieces, axis=1).reset_index()\n",
    "\n",
    "# Tri: ann√©e croissante puis ordre custom T‚ÜíS‚ÜíAn\n",
    "def extract_year_safe(x):\n",
    "    try:\n",
    "        return int(re.search(r\"\\d{4}\", str(x)).group(0))\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "wide[\"__Year\"] = wide[\"P√©riode\"].apply(extract_year_safe)\n",
    "wide[\"__PeriodeOrd\"] = pd.Categorical(wide[\"P√©riode_d√©tail\"], categories=ordre_pd, ordered=True)\n",
    "wide = wide.sort_values([\"__Year\",\"__PeriodeOrd\",\"P√©riode_d√©tail\",\"P√©riode\"], na_position=\"last\")\n",
    "\n",
    "# --- NOUVEAU : r√©ordonner les colonnes par Ticker puis variables ---\n",
    "fixed_prefix = [\"P√©riode_d√©tail\", \"P√©riode\"]\n",
    "candidate = [f\"{t}_{v}\" for t in tickers for v in vars_keep]\n",
    "ordered_data_cols = [c for c in candidate if c in wide.columns]  # garde l'ordre souhait√©, ignore si colonne manquante\n",
    "other_cols = [c for c in wide.columns if c not in fixed_prefix + ordered_data_cols + [\"__Year\",\"__PeriodeOrd\"]]\n",
    "wide = wide[fixed_prefix + ordered_data_cols + other_cols]\n",
    "# -------------------------------------------------------------------\n",
    "\n",
    "# Nettoyage\n",
    "wide = wide.drop(columns=[\"__Year\",\"__PeriodeOrd\"], errors=\"ignore\")\n",
    "\n",
    "# Export\n",
    "wide.to_csv(DST, index=False)\n",
    "print(f\"‚úÖ Pivot compl√©t√© & tri√© (T‚ÜíS‚ÜíAn) et colonnes group√©es par ticker export√© : {DST}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fe80e4e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Fichiers cr√©√©s :\n",
      "- rapport_brvm_Annuel.csv\n",
      "- rapport_brvm_Trimestriel.csv\n",
      "- rapport_brvm_Semestriel.csv\n",
      "- rapport_brvm_decoupe.xlsx (avec 3 onglets)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "SRC = \"rapport_brvm_complet_uniformise3.csv\"\n",
    "\n",
    "AN_DST = \"rapport_brvm_Annuel.csv\"\n",
    "TRI_DST = \"rapport_brvm_Trimestriel.csv\"\n",
    "SEM_DST = \"rapport_brvm_Semestriel.csv\"\n",
    "\n",
    "XLSX_DST = \"rapport_brvm_decoupe.xlsx\"\n",
    "\n",
    "# charge la table pivot large\n",
    "df = pd.read_csv(SRC)\n",
    "\n",
    "# garde uniquement les colonnes attendues au cas o√π\n",
    "assert \"P√©riode_d√©tail\" in df.columns, \"Colonne 'P√©riode_d√©tail' manquante dans le fichier source.\"\n",
    "\n",
    "# filtres\n",
    "annuel = df[df[\"P√©riode_d√©tail\"] == \"An\"].copy()\n",
    "trimestriel = df[df[\"P√©riode_d√©tail\"].isin([\"T1\",\"T2\",\"T3\",\"T4\"])].copy()\n",
    "semestriel = df[df[\"P√©riode_d√©tail\"].isin([\"S1\",\"S2\"])].copy()\n",
    "\n",
    "# export CSV\n",
    "annuel.to_csv(AN_DST, index=False)\n",
    "trimestriel.to_csv(TRI_DST, index=False)\n",
    "semestriel.to_csv(SEM_DST, index=False)\n",
    "\n",
    "# export Excel avec 3 onglets\n",
    "with pd.ExcelWriter(XLSX_DST, engine=\"openpyxl\") as writer:\n",
    "    annuel.to_excel(writer, sheet_name=\"Annuel\", index=False)\n",
    "    trimestriel.to_excel(writer, sheet_name=\"Trimestriel\", index=False)\n",
    "    semestriel.to_excel(writer, sheet_name=\"Semestriel\", index=False)\n",
    "\n",
    "print(\"‚úÖ Fichiers cr√©√©s :\")\n",
    "print(f\"- {AN_DST}\")\n",
    "print(f\"- {TRI_DST}\")\n",
    "print(f\"- {SEM_DST}\")\n",
    "print(f\"- {XLSX_DST} (avec 3 onglets)\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
