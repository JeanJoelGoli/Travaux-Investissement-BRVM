{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0d8dbf74",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò ABJC\n",
      "üìò BICB\n",
      "üìò BICC\n",
      "üìò BNBC\n",
      "üìò BOAB\n",
      "üìò BOABF\n",
      "üìò BOAC\n",
      "üìò BOAM\n",
      "üìò BOAN\n",
      "üìò BOAS\n",
      "üìò CABC\n",
      "üìò CBIBF\n",
      "üìò CFAC\n",
      "üìò CIEC\n",
      "üìò ECOC\n",
      "üìò ETIT\n",
      "üìò FTSC\n",
      "üìò LNBB\n",
      "üìò NEIC\n",
      "üìò NSBC\n",
      "üìò NTLC\n",
      "üìò ONTBF\n",
      "üìò ORAC\n",
      "üìò ORGT\n",
      "üìò PALC\n",
      "üìò PRSC\n",
      "üìò SAFC\n",
      "üìò SCRC\n",
      "üìò SDCC\n",
      "üìò SDSC\n",
      "üìò SEMC\n",
      "üìò SGBC\n",
      "üìò SHEC\n",
      "üìò SIBC\n",
      "üìò SICC\n",
      "üìò SIVC\n",
      "üìò SLBC\n",
      "üìò SMBC\n",
      "üìò SNTS\n",
      "üìò SOGC\n",
      "üìò SPHC\n",
      "üìò STAC\n",
      "üìò STBC\n",
      "üìò SVOC\n",
      "üìò TTLC\n",
      "üìò TTLS\n",
      "üìò UNLC\n",
      "üìò UNXC\n",
      "‚úÖ societes_details.csv √©crit\n",
      "‚ÑπÔ∏è Aucun fractionnement trouv√©\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup, NavigableString\n",
    "from urllib.parse import urljoin\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# ---- tickers ----\n",
    "TICKERS = [\n",
    "    \"ABJC\",\"BICB\",\"BICC\",\"BNBC\",\"BOAB\",\"BOABF\",\"BOAC\",\"BOAM\",\"BOAN\",\"BOAS\",\n",
    "    \"CABC\",\"CBIBF\",\"CFAC\",\"CIEC\",\"ECOC\",\"ETIT\",\"FTSC\",\"LNBB\",\"NEIC\",\"NSBC\",\n",
    "    \"NTLC\",\"ONTBF\",\"ORAC\",\"ORGT\",\"PALC\",\"PRSC\",\"SAFC\",\"SCRC\",\"SDCC\",\"SDSC\",\n",
    "    \"SEMC\",\"SGBC\",\"SHEC\",\"SIBC\",\"SICC\",\"SIVC\",\"SLBC\",\"SMBC\",\"SNTS\",\"SOGC\",\n",
    "    \"SPHC\",\"STAC\",\"STBC\",\"SVOC\",\"TTLC\",\"TTLS\",\"UNLC\",\"UNXC\"\n",
    "]\n",
    "\n",
    "BASE = \"https://www.richbourse.com\"\n",
    "DETAIL_URL = BASE + \"/common/apprendre/details-societe/{ticker}\"\n",
    "UA = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6) AppleWebKit/537.36 \"\n",
    "      \"(KHTML, like Gecko) Chrome/126.0 Safari/537.36\")\n",
    "\n",
    "HEADER_TAGS = (\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"strong\",\"b\")\n",
    "\n",
    "def norm(s: str) -> str:\n",
    "    return (s or \"\").replace(\"\\xa0\", \" \").replace(\"\\u2019\", \"'\").strip()\n",
    "\n",
    "def clean_join(lines):\n",
    "    return re.sub(r\"\\s{2,}\", \" \", \" \".join([norm(x) for x in lines if norm(x)]))\n",
    "\n",
    "def target_content(soup: BeautifulSoup) -> BeautifulSoup:\n",
    "    \"\"\"\n",
    "    Restreint au bloc central (en dessous du H1 de la soci√©t√©).\n",
    "    \"\"\"\n",
    "    h1 = soup.find(lambda t: getattr(t, \"name\", \"\") in (\"h1\",\"h2\") and \"D√©tails\" not in t.get_text())\n",
    "    # fallback: bloc contenant \"Soci√©t√© :\" quelque part\n",
    "    if not h1:\n",
    "        node = soup.find(lambda t: getattr(t, \"name\", \"\") in (\"div\",\"section\",\"article\") and \"Soci√©t√© :\" in t.get_text())\n",
    "        return node or soup\n",
    "    # prendre sa section/parente proche\n",
    "    for anc in h1.parents:\n",
    "        if getattr(anc, \"name\", \"\") in (\"section\",\"div\",\"main\"):\n",
    "            return anc\n",
    "    return soup\n",
    "\n",
    "def extract_pairs(content: BeautifulSoup) -> dict:\n",
    "    \"\"\"\n",
    "    Robustesse ++ : cherche les paires sous 2 formes :\n",
    "    1) <p><strong>Cl√© :</strong> Valeur</p>\n",
    "    2) Texte brut \"Cl√© : Valeur\" dans <p>/<div>/<li>\n",
    "    \"\"\"\n",
    "    wanted = {\n",
    "        \"Soci√©t√©\": \"Societe\",\n",
    "        \"Secteur d'activit√©\": \"Secteur d'activit√©\",\n",
    "        \"Pays\": \"Pays\",\n",
    "        \"Introduction √† la BRVM\": \"Introduction √† la BRVM\",\n",
    "        \"Nombre de titres\": \"Nombre de titres\",\n",
    "        \"Flottant\": \"Flottant\",\n",
    "        \"Site Web\": \"Site Web\",\n",
    "        \"T√©l√©phone\": \"T√©l√©phone\",\n",
    "    }\n",
    "    out = {v: \"\" for v in wanted.values()}\n",
    "\n",
    "    # 1) patron <strong>cl√©</strong> valeur\n",
    "    for p in content.find_all(\"p\"):\n",
    "        st = p.find(\"strong\")\n",
    "        if not st:\n",
    "            continue\n",
    "        key = norm(st.get_text()).rstrip(\":\")\n",
    "        if key in wanted:\n",
    "            val = norm(p.get_text(\" \", strip=True))\n",
    "            sval = norm(st.get_text())\n",
    "            if val.startswith(sval):\n",
    "                val = norm(val[len(sval):].lstrip(\":\"))\n",
    "            out[wanted[key]] = val\n",
    "\n",
    "    # 2) texte brut cl√©: valeur\n",
    "    nodes = list(content.find_all([\"p\",\"div\",\"li\"]))\n",
    "    for n in nodes:\n",
    "        txt = norm(n.get_text(\" \", strip=True))\n",
    "        for k, mapped in wanted.items():\n",
    "            # Ex.: \"Soci√©t√© : xxx\"\n",
    "            m = re.search(rf\"\\b{k}\\s*:\\s*(.+)\", txt)\n",
    "            if m and not out[mapped]:\n",
    "                out[mapped] = norm(m.group(1))\n",
    "\n",
    "    return out\n",
    "\n",
    "def section_text(content: BeautifulSoup, titles, stops):\n",
    "    \"\"\"\n",
    "    Lit le texte apr√®s un titre (Pr√©sentation / D√©terminants...) via next_siblings\n",
    "    jusqu'au prochain titre stop.\n",
    "    \"\"\"\n",
    "    titles = tuple(norm(t) for t in titles)\n",
    "    stops  = tuple(norm(s) for s in stops)\n",
    "    header = content.find(lambda t: getattr(t, \"name\",\"\") in HEADER_TAGS and any(tv in norm(t.get_text()) for tv in titles))\n",
    "    if not header:\n",
    "        return \"\"\n",
    "    out = []\n",
    "    for sib in header.next_siblings:\n",
    "        if isinstance(sib, NavigableString):\n",
    "            continue\n",
    "        name = getattr(sib, \"name\", \"\")\n",
    "        if name in HEADER_TAGS and any(st in norm(sib.get_text()) for st in stops):\n",
    "            break\n",
    "        if name in (\"p\",\"ul\",\"ol\",\"div\",\"section\",\"article\"):\n",
    "            if name in (\"ul\",\"ol\"):\n",
    "                out.extend([li.get_text(\" \", strip=True) for li in sib.find_all(\"li\")])\n",
    "            else:\n",
    "                out.append(sib.get_text(\" \", strip=True))\n",
    "    return clean_join(out)\n",
    "\n",
    "def extract_actionnaires(content: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Capte 'Nom (xx,yy%)' m√™me si le nom contient des parenth√®ses (ex: Public (BRVM)).\n",
    "    \"\"\"\n",
    "    text = content.get_text(\"\\n\", strip=True)\n",
    "    items = re.findall(r\"[A-Za-z√Ä-√ø0-9\\-\\.'& ()]+?\\(\\d+[,\\.]?\\d*%\\)\", text)\n",
    "    seen, uniq = set(), []\n",
    "    for it in map(norm, items):\n",
    "        if it not in seen:\n",
    "            uniq.append(it); seen.add(it)\n",
    "    return \"; \".join(uniq)\n",
    "\n",
    "def extract_logo(content: BeautifulSoup) -> str:\n",
    "    \"\"\"\n",
    "    Prend de pr√©f√©rence un logo dans /ressources/uploads/profil-societe/.\n",
    "    Fallback: premier <img> non traqueur hors header/footer.\n",
    "    \"\"\"\n",
    "    img = content.find(\"img\", src=re.compile(r\"/ressources/uploads/profil-societe/\", re.I))\n",
    "    if img:\n",
    "        return urljoin(BASE, img[\"src\"])\n",
    "    for img in content.find_all(\"img\", src=True):\n",
    "        src = img[\"src\"].lower()\n",
    "        if any(bad in src for bad in [\"cleardot\",\"google\",\"analytics\",\"rb_logo_long\"]):\n",
    "            continue\n",
    "        # ignorer 1x1\n",
    "        w = (img.get(\"width\") or \"\").strip()\n",
    "        h = (img.get(\"height\") or \"\").strip()\n",
    "        if w.isdigit() and h.isdigit() and (w == \"1\" or h == \"1\"):\n",
    "            continue\n",
    "        return urljoin(BASE, img[\"src\"])\n",
    "    return \"\"\n",
    "\n",
    "def parse_fractionnements(content: BeautifulSoup, ticker: str) -> pd.DataFrame:\n",
    "    for tbl in content.find_all(\"table\"):\n",
    "        thead = tbl.find(\"thead\")\n",
    "        if not thead:\n",
    "            continue\n",
    "        headers = [norm(th.get_text()) for th in thead.find_all(\"th\")]\n",
    "        if \"Date\" in headers and (\"Parit√©\" in headers or \"Parite\" in headers):\n",
    "            tbody = tbl.find(\"tbody\")\n",
    "            if not tbody:\n",
    "                continue\n",
    "            rows = []\n",
    "            for tr in tbody.find_all(\"tr\"):\n",
    "                tds = tr.find_all(\"td\")\n",
    "                if not tds:\n",
    "                    continue\n",
    "                date = norm(tds[0].get_text(\" \", strip=True)) if len(tds)>0 else \"\"\n",
    "                parite = norm(tds[1].get_text(\" \", strip=True)) if len(tds)>1 else \"\"\n",
    "                file_url = \"\"\n",
    "                if len(tds) > 2:\n",
    "                    a = tds[2].find(\"a\", href=True)\n",
    "                    if a:\n",
    "                        file_url = urljoin(BASE, a[\"href\"])\n",
    "                rows.append({\"Ticker\": ticker, \"Date\": date, \"Parite\": parite, \"FichierURL\": file_url})\n",
    "            return pd.DataFrame(rows)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "async def robust_goto(page, url):\n",
    "    await page.goto(url, timeout=60000)\n",
    "    await page.wait_for_load_state(\"domcontentloaded\")\n",
    "    try:\n",
    "        await page.wait_for_load_state(\"networkidle\", timeout=8000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "async def scrape_one(page, ticker):\n",
    "    url = DETAIL_URL.format(ticker=ticker)\n",
    "    await robust_goto(page, url)\n",
    "    soup = BeautifulSoup(await page.content(), \"html.parser\")\n",
    "\n",
    "    content = target_content(soup)  # zone centrale seulement\n",
    "\n",
    "    pairs = extract_pairs(content)\n",
    "    presentation = section_text(\n",
    "        content,\n",
    "        titles=(\"Pr√©sentation\",),\n",
    "        stops=(\"D√©terminants\", \"Derniers fractionnements\", \"Principaux actionnaires\")\n",
    "    )\n",
    "    determinants = section_text(\n",
    "        content,\n",
    "        titles=(\"D√©terminants Sectoriel\",\"D√©terminants sectoriel\"),\n",
    "        stops=(\"Derniers fractionnements\",\"Principaux actionnaires\",\"Pr√©sentation\")\n",
    "    )\n",
    "    actionnaires = extract_actionnaires(content)\n",
    "    logo = extract_logo(content)\n",
    "    frac = parse_fractionnements(content, ticker)\n",
    "\n",
    "    row = {\n",
    "        \"Ticker\": ticker,\n",
    "        \"Societe\": pairs.get(\"Societe\",\"\"),\n",
    "        \"Secteur d'activit√©\": pairs.get(\"Secteur d'activit√©\",\"\"),\n",
    "        \"Pays\": pairs.get(\"Pays\",\"\"),\n",
    "        \"Introduction √† la BRVM\": pairs.get(\"Introduction √† la BRVM\",\"\"),\n",
    "        \"Nombre de titres\": pairs.get(\"Nombre de titres\",\"\"),\n",
    "        \"Flottant\": pairs.get(\"Flottant\",\"\"),\n",
    "        \"Site Web\": pairs.get(\"Site Web\",\"\"),\n",
    "        \"T√©l√©phone\": pairs.get(\"T√©l√©phone\",\"\"),\n",
    "        \"LogoURL\": logo,\n",
    "        \"Presentation\": presentation,\n",
    "        \"DeterminantsSectoriels\": determinants,\n",
    "        \"ActionnairesPrincipaux\": actionnaires,\n",
    "        \"SourceURL\": url,\n",
    "    }\n",
    "    return row, frac\n",
    "\n",
    "async def main():\n",
    "    rows, fracs = [], []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False, args=[\"--disable-blink-features=AutomationControlled\"])\n",
    "        context = await browser.new_context(user_agent=UA, locale=\"fr-FR\")\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for t in TICKERS:\n",
    "            try:\n",
    "                print(\"üìò\", t)\n",
    "                r, df_frac = await scrape_one(page, t)\n",
    "                rows.append(r)\n",
    "                if not df_frac.empty:\n",
    "                    fracs.append(df_frac)\n",
    "                time.sleep(0.25)\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå {t}: {e}\")\n",
    "                rows.append({\n",
    "                    \"Ticker\": t, \"Societe\":\"\", \"Secteur d'activit√©\":\"\", \"Pays\":\"\",\n",
    "                    \"Introduction √† la BRVM\":\"\", \"Nombre de titres\":\"\", \"Flottant\":\"\",\n",
    "                    \"Site Web\":\"\", \"T√©l√©phone\":\"\", \"LogoURL\":\"\",\n",
    "                    \"Presentation\":\"\", \"DeterminantsSectoriels\":\"\",\n",
    "                    \"ActionnairesPrincipaux\":\"\", \"SourceURL\": DETAIL_URL.format(ticker=t)\n",
    "                })\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    # d√©tails soci√©t√©s\n",
    "    cols = [\"Ticker\",\"Societe\",\"Secteur d'activit√©\",\"Pays\",\"Introduction √† la BRVM\",\n",
    "            \"Nombre de titres\",\"Flottant\",\"Site Web\",\"T√©l√©phone\",\"LogoURL\",\n",
    "            \"Presentation\",\"DeterminantsSectoriels\",\"ActionnairesPrincipaux\",\"SourceURL\"]\n",
    "    df = pd.DataFrame(rows)\n",
    "    for c in cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"\n",
    "    df = df[cols]\n",
    "    df.to_csv(\"societes_details.csv\", index=False)\n",
    "    print(\"‚úÖ societes_details.csv √©crit\")\n",
    "\n",
    "    # fractionnements\n",
    "    if fracs:\n",
    "        dff = pd.concat(fracs, ignore_index=True)\n",
    "        dff.to_csv(\"societes_fractionnements.csv\", index=False)\n",
    "        print(\"‚úÖ societes_fractionnements.csv √©crit\")\n",
    "    else:\n",
    "        print(\"‚ÑπÔ∏è Aucun fractionnement trouv√©\")\n",
    "\n",
    "# Notebook/Jupyter :\n",
    "import asyncio\n",
    "await main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
