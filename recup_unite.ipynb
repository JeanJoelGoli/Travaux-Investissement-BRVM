{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "71bcb6d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ABJC ...\n",
      "üìä BICB ...\n",
      "‚úÖ Fichier rapport_brvm_complet.csv g√©n√©r√© (35 lignes)\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import re\n",
    "import time\n",
    "import random\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "TICKERS = [\n",
    "    \"ABJC\",\"BICB\"\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://www.richbourse.com/investisseur/rapport-activite/index/{ticker}/{type_}/{periode}\"\n",
    "\n",
    "def clean_cell(val: str) -> str:\n",
    "    if not val:\n",
    "        return \"\"\n",
    "    val = re.split(r\"\\(\", val)[0]        # coupe la variation (%)\n",
    "    val = (val.replace(\"\\xa0\", \" \")\n",
    "              .replace(\" \", \"\")\n",
    "              .replace(\",\", \"\"))\n",
    "    return val.strip()\n",
    "\n",
    "def detect_unit_footer(soup: BeautifulSoup) -> str:\n",
    "    patterns = [\n",
    "        r\"donn[√©e]es?.*xof\",\n",
    "        r\"donn[√©e]es?.*francs?\\s*cfa\",\n",
    "        r\"donn[√©e]es?.*milliers.*xof\",\n",
    "        r\"en\\s+milliers\\s+de\\s+xof\"\n",
    "    ]\n",
    "    for el in soup.select(\"p, small, div, span, footer\")[::-1]:\n",
    "        txt = el.get_text(\" \", strip=True)\n",
    "        if not txt:\n",
    "            continue\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, txt, flags=re.I):\n",
    "                return txt\n",
    "    return \"\"\n",
    "\n",
    "def period_label(type_: str, periode: str) -> str:\n",
    "    t = type_.lower()\n",
    "    if t.startswith(\"trimes\"):\n",
    "        return f\"T{periode}\"\n",
    "    if t.startswith(\"semest\"):\n",
    "        return \"S1\" if periode == \"1\" else \"S2\"\n",
    "    return \"An\"\n",
    "\n",
    "async def scrape_report(page, ticker: str, type_: str, periode: str) -> pd.DataFrame:\n",
    "    url = BASE_URL.format(ticker=ticker, type_=type_, periode=periode)\n",
    "    await page.goto(url, timeout=60000)\n",
    "\n",
    "    try:\n",
    "        await page.wait_for_selector(\"table.table\", timeout=8000)\n",
    "    except:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    html_page = await page.content()\n",
    "    soup_page = BeautifulSoup(html_page, \"html.parser\")\n",
    "    table = soup_page.select_one(\"table.table\")\n",
    "    if table is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # headers\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [th.get_text(strip=True) for th in thead.find_all(\"th\")]\n",
    "    else:\n",
    "        first_tr = table.find(\"tbody\").find(\"tr\")\n",
    "        headers = [td.get_text(strip=True) for td in first_tr.find_all(\"td\")]\n",
    "\n",
    "    if \"P√©riode\" not in headers:\n",
    "        headers = [\"P√©riode\"] + headers\n",
    "\n",
    "    # rows\n",
    "    rows = []\n",
    "    body = table.find(\"tbody\")\n",
    "    if body:\n",
    "        for tr in body.find_all(\"tr\"):\n",
    "            tds = [td.get_text(\" \", strip=True) for td in tr.find_all(\"td\")]\n",
    "            if not tds:\n",
    "                continue\n",
    "            if len(tds) == len(headers) - 1 and headers[0] == \"P√©riode\":\n",
    "                tds = [\"\"] + tds  # si la p√©riode n'est pas rendue en colonne\n",
    "            row = {headers[i]: clean_cell(tds[i]) for i in range(min(len(headers), len(tds)))}\n",
    "            rows.append(row)\n",
    "\n",
    "    df = pd.DataFrame(rows)\n",
    "    if df.empty:\n",
    "        return df\n",
    "\n",
    "    # note d'unit√© en bas de page\n",
    "    unit_phrase = detect_unit_footer(soup_page)\n",
    "\n",
    "    # colonnes demand√©es\n",
    "    type_norm = \"Annuel\" if type_.lower().startswith(\"annuel\") else (\"Semestriel\" if type_.lower().startswith(\"semest\") else \"Trimestriel\")\n",
    "    df[\"Ticker\"] = ticker\n",
    "    df[\"Type\"] = type_norm\n",
    "    df[\"P√©riode_d√©tail\"] = period_label(type_norm, periode)\n",
    "    df[\"en XOF\"] = unit_phrase  # remplace \"unit√©\"\n",
    "\n",
    "    # ordre colonnes\n",
    "    front = [\"Ticker\", \"Type\", \"P√©riode_d√©tail\", \"P√©riode\", \"en XOF\"]\n",
    "    others = [c for c in df.columns if c not in front]\n",
    "    return df[front + others]\n",
    "\n",
    "async def main(output_csv: str = \"rapport_brvm_complet.csv\", headless: bool = True):\n",
    "    all_parts = []\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=headless)\n",
    "        try:\n",
    "            context = await browser.new_context(storage_state=\"cookies.json\")\n",
    "        except:\n",
    "            context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "\n",
    "        for ticker in TICKERS:\n",
    "            print(f\"üìä {ticker} ...\")\n",
    "            dfs = []\n",
    "            # Annuel\n",
    "            dfs.append(await scrape_report(page, ticker, \"Annuel\", \"1\"))\n",
    "            # Semestriels\n",
    "            dfs.append(await scrape_report(page, ticker, \"Semestriel\", \"1\"))\n",
    "            dfs.append(await scrape_report(page, ticker, \"Semestriel\", \"2\"))\n",
    "            # Trimestriels\n",
    "            for i in range(1, 4+1):\n",
    "                dfs.append(await scrape_report(page, ticker, \"Trimestriel\", str(i)))\n",
    "\n",
    "            part = pd.concat([d for d in dfs if isinstance(d, pd.DataFrame) and not d.empty],\n",
    "                             ignore_index=True) if dfs else pd.DataFrame()\n",
    "            if part.empty:\n",
    "                all_parts.append(pd.DataFrame([{\n",
    "                    \"Ticker\": ticker, \"Type\": \"Aucune donn√©e\",\n",
    "                    \"P√©riode_d√©tail\": \"\", \"P√©riode\": \"\", \"en XOF\": \"\"\n",
    "                }]))\n",
    "            else:\n",
    "                all_parts.append(part)\n",
    "\n",
    "            time.sleep(0.5 + random.random()*0.7)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    df_final = pd.concat(all_parts, ignore_index=True) if all_parts else pd.DataFrame()\n",
    "    if df_final.empty:\n",
    "        print(\"‚ö†Ô∏è Aucun rapport r√©cup√©r√©\")\n",
    "        return df_final\n",
    "\n",
    "    first_cols = [\"Ticker\", \"Type\", \"P√©riode_d√©tail\", \"P√©riode\", \"en XOF\"]\n",
    "    df_final = df_final[first_cols + [c for c in df_final.columns if c not in first_cols]]\n",
    "    df_final.to_csv(output_csv, index=False)\n",
    "    print(f\"‚úÖ Fichier {output_csv} g√©n√©r√© ({len(df_final)} lignes)\")\n",
    "    return df_final\n",
    "\n",
    "# Exemple d'ex√©cution dans Jupyter :\n",
    "df = await main(headless=False)\n",
    "# df.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d023fe5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
