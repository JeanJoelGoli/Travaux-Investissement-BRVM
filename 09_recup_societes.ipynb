{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c3cd5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import os\n",
    "import time\n",
    "import math\n",
    "import json\n",
    "import traceback\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# -----------------------------\n",
    "# Paramètres\n",
    "# -----------------------------\n",
    "\n",
    "TICKERS = [\n",
    "    \"ABJC\",\"BICB\",\"BICC\",\"BNBC\",\"BOAB\",\"BOABF\",\"BOAC\",\"BOAM\",\"BOAN\",\"BOAS\",\n",
    "    \"CABC\",\"CBIBF\",\"CFAC\",\"CIEC\",\"ECOC\",\"ETIT\",\"FTSC\",\"LNBB\",\"NEIC\",\"NSBC\",\n",
    "    \"NTLC\",\"ONTBF\",\"ORAC\",\"ORGT\",\"PALC\",\"PRSC\",\"SAFC\",\"SCRC\",\"SDCC\",\"SDSC\",\n",
    "    \"SEMC\",\"SGBC\",\"SHEC\",\"SIBC\",\"SICC\",\"SIVC\",\"SLBC\",\"SMBC\",\"SNTS\",\"SOGC\",\n",
    "    \"SPHC\",\"STAC\",\"STBC\",\"SVOC\",\"TTLC\",\"TTLS\",\"UNLC\",\"UNXC\"\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://www.richbourse.com/common/apprendre/details-societe/{ticker}\"\n",
    "\n",
    "# Fichiers de sortie\n",
    "OUT_CSV = \"richbourse_societes.csv\"\n",
    "OUT_XLSX = \"richbourse_societes.xlsx\"\n",
    "\n",
    "# Dossier des logos\n",
    "LOGO_DIR = \"logos_richbourse\"\n",
    "\n",
    "# Colonnes demandées (ordre)\n",
    "OUTPUT_COLUMNS = [\n",
    "    \"ticker\",\n",
    "    \"Société\",\n",
    "    \"Secteur d'activité\",\n",
    "    \"Pays\",\n",
    "    \"Introduction à la BRVM\",\n",
    "    \"Nombre de titres\",\n",
    "    \"Flottant\",\n",
    "    \"Site Web\",\n",
    "    \"Présentation\",\n",
    "    \"Déterminants Sectoriel\",\n",
    "    \"Logo_Path\"\n",
    "]\n",
    "\n",
    "# -----------------------------\n",
    "# Session HTTP robuste\n",
    "# -----------------------------\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/126.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.9\"\n",
    "    })\n",
    "    adapter = requests.adapters.HTTPAdapter(max_retries=3)\n",
    "    s.mount(\"http://\", adapter)\n",
    "    s.mount(\"https://\", adapter)\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "# -----------------------------\n",
    "# Utils parsing\n",
    "# -----------------------------\n",
    "\n",
    "LABEL_KEYS = {\n",
    "    \"Société\": \"Société\",\n",
    "    \"Secteur d'activité\": \"Secteur d'activité\",\n",
    "    \"Pays\": \"Pays\",\n",
    "    \"Introduction à la BRVM\": \"Introduction à la BRVM\",\n",
    "    \"Nombre de titres\": \"Nombre de titres\",\n",
    "    \"Flottant\": \"Flottant\",\n",
    "    \"Site Web\": \"Site Web\",\n",
    "}\n",
    "\n",
    "LABELS_LOWER = {k.lower(): v for k, v in LABEL_KEYS.items()}\n",
    "\n",
    "HEADER_PRESENTATION_RE = re.compile(r\"^\\s*Présentation\\s*$\", re.IGNORECASE)\n",
    "HEADER_DETERMINANTS_RE = re.compile(r\"^\\s*Déterminants?\\s+Sectoriel\", re.IGNORECASE)\n",
    "HEADER_STOP_WORDS = [\n",
    "    re.compile(r\"^\\s*Principaux actionnaires\", re.IGNORECASE),\n",
    "    re.compile(r\"^\\s*Actionnariat\", re.IGNORECASE),\n",
    "    re.compile(r\"^\\s*Contacts?\", re.IGNORECASE),\n",
    "]\n",
    "\n",
    "def normalize_space(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s).strip()\n",
    "\n",
    "def extract_fields_block(soup: BeautifulSoup) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Cherche les paragraphes/divs qui commencent par \"Société :\", \"Pays :\", etc.\n",
    "    Renvoie un dict {label_canonique: valeur}\n",
    "    \"\"\"\n",
    "    data = {}\n",
    "    candidates = soup.select(\"p, li, div\")\n",
    "    for tag in candidates:\n",
    "        text = normalize_space(\" \".join(tag.stripped_strings))\n",
    "        if \":\" not in text:\n",
    "            continue\n",
    "        left, right = text.split(\":\", 1)\n",
    "        left_n = left.strip().lower()\n",
    "        if left_n in LABELS_LOWER:\n",
    "            canon = LABELS_LOWER[left_n]\n",
    "            val = right.strip()\n",
    "            # Nettoyages fréquents\n",
    "            val = re.sub(r\"\\s{2,}\", \" \", val)\n",
    "            data[canon] = val\n",
    "    return data\n",
    "\n",
    "def _collect_text_until(next_node, stop_patterns: List[re.Pattern]) -> str:\n",
    "    \"\"\"\n",
    "    Concatène le texte des nœuds suivants jusqu'à tomber sur un en-tête stop.\n",
    "    \"\"\"\n",
    "    parts = []\n",
    "    node = next_node\n",
    "    while node:\n",
    "        # Si on tombe sur un header identifiable, on stoppe\n",
    "        if hasattr(node, \"get_text\"):\n",
    "            t = normalize_space(node.get_text(separator=\" \", strip=True))\n",
    "            if any(p.search(t) for p in [HEADER_PRESENTATION_RE, HEADER_DETERMINANTS_RE] + HEADER_STOP_WORDS):\n",
    "                # Si c'est nous-même au tout début, on continue; sinon on s'arrête\n",
    "                if not parts:  # évite de stopper sur le même header trouvé\n",
    "                    node = node.find_next_sibling()\n",
    "                    continue\n",
    "                break\n",
    "            # On ne garde que le texte \"paragraphe-like\"\n",
    "            if node.name in (\"p\", \"div\", \"li\"):\n",
    "                if t and not any(p.search(t) for p in [HEADER_PRESENTATION_RE, HEADER_DETERMINANTS_RE]):\n",
    "                    parts.append(t)\n",
    "        node = node.find_next_sibling()\n",
    "    # Dedup lignes trop proches\n",
    "    uniq = []\n",
    "    for p in parts:\n",
    "        if not uniq or p != uniq[-1]:\n",
    "            uniq.append(p)\n",
    "    return \"\\n\".join(uniq).strip()\n",
    "\n",
    "def extract_section_text(soup: BeautifulSoup, header_regex: re.Pattern) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Trouve un titre (h*, strong, b) qui matche header_regex et renvoie le texte\n",
    "    des paragraphes suivants jusqu'au prochain header pertinent.\n",
    "    \"\"\"\n",
    "    # Cherche un header classique\n",
    "    header = None\n",
    "    for tag in soup.find_all(True):\n",
    "        if tag.name in (\"h1\",\"h2\",\"h3\",\"h4\",\"h5\",\"h6\",\"strong\",\"b\"):\n",
    "            txt = normalize_space(tag.get_text(separator=\" \", strip=True))\n",
    "            if header_regex.search(txt):\n",
    "                header = tag\n",
    "                break\n",
    "    if not header:\n",
    "        # fallback: chercher texte brut\n",
    "        header = soup.find(string=header_regex)\n",
    "        header = header.parent if header and hasattr(header, \"parent\") else None\n",
    "    if not header:\n",
    "        return None\n",
    "    text = _collect_text_until(header.find_next_sibling(), stop_patterns=HEADER_STOP_WORDS)\n",
    "    return text if text else None\n",
    "\n",
    "def find_logo_url(soup: BeautifulSoup) -> Optional[str]:\n",
    "    \"\"\"\n",
    "    Essaie de localiser un logo plausible.\n",
    "    Heuristique: première image dans la colonne de gauche, ou img dont src contient 'logo' ou 'logos'.\n",
    "    \"\"\"\n",
    "    imgs = soup.find_all(\"img\")\n",
    "    best = None\n",
    "    for img in imgs:\n",
    "        src = img.get(\"src\") or \"\"\n",
    "        alt = (img.get(\"alt\") or \"\").lower()\n",
    "        if not src:\n",
    "            continue\n",
    "        # Compléter src relatif\n",
    "        if src.startswith(\"//\"):\n",
    "            src = \"https:\" + src\n",
    "        elif src.startswith(\"/\"):\n",
    "            src = \"https://www.richbourse.com\" + src\n",
    "        # Heuristiques\n",
    "        score = 0\n",
    "        s_low = src.lower()\n",
    "        if \"logo\" in s_low or \"logos\" in s_low:\n",
    "            score += 2\n",
    "        if \"apprendre\" in s_low or \"societe\" in s_low:\n",
    "            score += 1\n",
    "        if alt and (\"logo\" in alt or \"soc\" in alt):\n",
    "            score += 1\n",
    "        # dimensions (si présentes)\n",
    "        try:\n",
    "            w = int(img.get(\"width\") or 0)\n",
    "            h = int(img.get(\"height\") or 0)\n",
    "            if w >= 80 and h >= 40:\n",
    "                score += 1\n",
    "        except Exception:\n",
    "            pass\n",
    "        if best is None or score > best[0]:\n",
    "            best = (score, src)\n",
    "    return best[1] if best else None\n",
    "\n",
    "def download_logo(url: str, ticker: str, out_dir: str = LOGO_DIR) -> Optional[str]:\n",
    "    os.makedirs(out_dir, exist_ok=True)\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        # extension\n",
    "        ext = \".png\"\n",
    "        ctype = r.headers.get(\"Content-Type\", \"\")\n",
    "        if \"jpeg\" in ctype or \"jpg\" in ctype:\n",
    "            ext = \".jpg\"\n",
    "        elif \"svg\" in ctype:\n",
    "            ext = \".svg\"\n",
    "        elif \"webp\" in ctype:\n",
    "            ext = \".webp\"\n",
    "        path = os.path.join(out_dir, f\"{ticker}{ext}\")\n",
    "        with open(path, \"wb\") as f:\n",
    "            f.write(r.content)\n",
    "        return path\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "# -----------------------------\n",
    "# Scraper principal\n",
    "# -----------------------------\n",
    "\n",
    "def scrape_one(ticker: str) -> Dict[str, Optional[str]]:\n",
    "    url = BASE_URL.format(ticker=ticker)\n",
    "    row = {col: None for col in OUTPUT_COLUMNS}\n",
    "    row[\"ticker\"] = ticker\n",
    "\n",
    "    try:\n",
    "        resp = SESSION.get(url, timeout=30)\n",
    "        resp.raise_for_status()\n",
    "        soup = BeautifulSoup(resp.text, \"lxml\")\n",
    "\n",
    "        # Champs simples\n",
    "        fields = extract_fields_block(soup)\n",
    "        for k, v in fields.items():\n",
    "            if k in row:\n",
    "                row[k] = v\n",
    "\n",
    "        # Présentation\n",
    "        pres = extract_section_text(soup, HEADER_PRESENTATION_RE)\n",
    "        if pres:\n",
    "            row[\"Présentation\"] = pres\n",
    "\n",
    "        # Déterminants Sectoriel (facultatif)\n",
    "        det = extract_section_text(soup, HEADER_DETERMINANTS_RE)\n",
    "        if det:\n",
    "            row[\"Déterminants Sectoriel\"] = det\n",
    "\n",
    "        # Logo\n",
    "        logo_url = find_logo_url(soup)\n",
    "        if logo_url:\n",
    "            logo_path = download_logo(logo_url, ticker)\n",
    "            if logo_path:\n",
    "                row[\"Logo_Path\"] = logo_path\n",
    "\n",
    "    except Exception as e:\n",
    "        # On loggue dans la console, et on laisse la ligne partiellement remplie\n",
    "        print(f\"[{ticker}] ERREUR: {e}\")\n",
    "        traceback.print_exc()\n",
    "\n",
    "    return row\n",
    "\n",
    "def scrape_all(tickers: List[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, t in enumerate(tickers, 1):\n",
    "        print(f\"[{i}/{len(tickers)}] {t} ...\")\n",
    "        rows.append(scrape_one(t))\n",
    "        # Petite pause pour être poli avec le site\n",
    "        time.sleep(0.7)\n",
    "    df = pd.DataFrame(rows, columns=OUTPUT_COLUMNS)\n",
    "    return df\n",
    "\n",
    "# -----------------------------\n",
    "# Main\n",
    "# -----------------------------\n",
    "\n",
    "def main():\n",
    "    df = scrape_all(TICKERS)\n",
    "\n",
    "    # Nettoyages légers\n",
    "    # Nombre de titres -> nombre (si possible)\n",
    "    def to_int(x):\n",
    "        if pd.isna(x):\n",
    "            return x\n",
    "        s = str(x).replace(\" \", \"\").replace(\"\\xa0\", \"\")\n",
    "        s = re.sub(r\"[^\\d]\", \"\", s)\n",
    "        return int(s) if s.isdigit() else x\n",
    "\n",
    "    df[\"Nombre de titres\"] = df[\"Nombre de titres\"].map(to_int)\n",
    "\n",
    "    # Flottant -> pourcentage brut (string conservé si format exotique)\n",
    "    def to_pct(x):\n",
    "        if pd.isna(x): return x\n",
    "        s = str(x).replace(\",\", \".\")\n",
    "        m = re.search(r\"(\\d+(?:\\.\\d+)?)\\s*%?\", s)\n",
    "        return float(m.group(1)) if m else x\n",
    "\n",
    "    df[\"Flottant\"] = df[\"Flottant\"].map(to_pct)\n",
    "\n",
    "    # Sauvegardes\n",
    "    df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    try:\n",
    "        df.to_excel(OUT_XLSX, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"Impossible d'écrire l'XLSX (openpyxl non installé ?) : {e}\")\n",
    "\n",
    "    print(\"\\nTerminé.\")\n",
    "    print(f\"CSV : {OUT_CSV}\")\n",
    "    print(f\"XLSX : {OUT_XLSX}\")\n",
    "    print(f\"Logos : {os.path.abspath(LOGO_DIR)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
