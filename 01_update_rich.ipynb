{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1963054b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 lignes ajoutées pour ABJC\n",
      "3 lignes ajoutées pour BICB\n",
      "3 lignes ajoutées pour BICC\n",
      "3 lignes ajoutées pour BNBC\n",
      "3 lignes ajoutées pour BOAB\n",
      "3 lignes ajoutées pour BOABF\n",
      "3 lignes ajoutées pour BOAC\n",
      "3 lignes ajoutées pour BOAM\n",
      "3 lignes ajoutées pour BOAN\n",
      "3 lignes ajoutées pour BOAS\n",
      "3 lignes ajoutées pour CABC\n",
      "3 lignes ajoutées pour CBIBF\n",
      "3 lignes ajoutées pour CFAC\n",
      "3 lignes ajoutées pour CIEC\n",
      "3 lignes ajoutées pour ECOC\n",
      "3 lignes ajoutées pour ETIT\n",
      "3 lignes ajoutées pour FTSC\n",
      "3 lignes ajoutées pour LNBB\n",
      "3 lignes ajoutées pour NEIC\n",
      "3 lignes ajoutées pour NSBC\n",
      "3 lignes ajoutées pour NTLC\n",
      "3 lignes ajoutées pour ONTBF\n",
      "3 lignes ajoutées pour ORAC\n",
      "3 lignes ajoutées pour ORGT\n",
      "3 lignes ajoutées pour PALC\n",
      "3 lignes ajoutées pour PRSC\n",
      "3 lignes ajoutées pour SAFC\n",
      "3 lignes ajoutées pour SCRC\n",
      "3 lignes ajoutées pour SDCC\n",
      "3 lignes ajoutées pour SDSC\n",
      "Aucune nouvelle ligne pour SEMC\n",
      "3 lignes ajoutées pour SGBC\n",
      "3 lignes ajoutées pour SHEC\n",
      "3 lignes ajoutées pour SIBC\n",
      "3 lignes ajoutées pour SICC\n",
      "3 lignes ajoutées pour SIVC\n",
      "3 lignes ajoutées pour SLBC\n",
      "3 lignes ajoutées pour SMBC\n",
      "3 lignes ajoutées pour SNTS\n",
      "3 lignes ajoutées pour SOGC\n",
      "3 lignes ajoutées pour SPHC\n",
      "3 lignes ajoutées pour STAC\n",
      "3 lignes ajoutées pour STBC\n",
      "Aucune nouvelle ligne pour SVOC\n",
      "3 lignes ajoutées pour TTLC\n",
      "3 lignes ajoutées pour TTLS\n",
      "3 lignes ajoutées pour UNLC\n",
      "3 lignes ajoutées pour UNXC\n",
      "Table mise à jour : combined_journalier.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import os\n",
    "import re\n",
    "import time\n",
    "import unicodedata\n",
    "import datetime as dt\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from requests.adapters import HTTPAdapter, Retry\n",
    "\n",
    "# -----------------------------\n",
    "# Config\n",
    "# -----------------------------\n",
    "TICKERS = [\n",
    "    \"ABJC\",\"BICB\",\"BICC\",\"BNBC\",\"BOAB\",\"BOABF\",\"BOAC\",\"BOAM\",\"BOAN\",\"BOAS\",\n",
    "    \"CABC\",\"CBIBF\",\"CFAC\",\"CIEC\",\"ECOC\",\"ETIT\",\"FTSC\",\"LNBB\",\"NEIC\",\"NSBC\",\n",
    "    \"NTLC\",\"ONTBF\",\"ORAC\",\"ORGT\",\"PALC\",\"PRSC\",\"SAFC\",\"SCRC\",\"SDCC\",\"SDSC\",\n",
    "    \"SEMC\",\"SGBC\",\"SHEC\",\"SIBC\",\"SICC\",\"SIVC\",\"SLBC\",\"SMBC\",\"SNTS\",\"SOGC\",\n",
    "    \"SPHC\",\"STAC\",\"STBC\",\"SVOC\",\"TTLC\",\"TTLS\",\"UNLC\",\"UNXC\"\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://www.richbourse.com/common/variation/historique/{ticker}/jour/0/{end}?page={page}\"\n",
    "HEADERS = {\n",
    "    \"User-Agent\": (\n",
    "        \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6) \"\n",
    "        \"AppleWebKit/537.36 (KHTML, like Gecko) Chrome/126.0 Safari/537.36\"\n",
    "    )\n",
    "}\n",
    "\n",
    "TARGET_COLS = [\n",
    "    \"Ticker\",\n",
    "    \"Date\",\n",
    "    \"Variation\",\n",
    "    \"Volume Devise Total\",\n",
    "    \"Cours Ajuste\",\n",
    "    \"Volume Ajuste Total\",\n",
    "    \"Cours Normal\",\n",
    "    \"Volume Normal Total\",\n",
    "]\n",
    "\n",
    "CSV_PATH = \"combined_journalier.csv\"\n",
    "MAX_PAGES = 60\n",
    "REQUEST_PAUSE = 0.30\n",
    "\n",
    "# -----------------------------\n",
    "# Utils\n",
    "# -----------------------------\n",
    "def _strip_all(s: str) -> str:\n",
    "    s = str(s).replace(\"\\ufeff\", \"\").replace(\"\\xa0\", \" \")\n",
    "    s = unicodedata.normalize(\"NFKC\", s)\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    return s\n",
    "\n",
    "def parse_date_any(series: pd.Series) -> pd.Series:\n",
    "    \"\"\"Parse dates possibles (ISO, dd/mm/YYYY, mm/dd/YYYY).\"\"\"\n",
    "    s = series.astype(str).str.strip()\n",
    "    d1 = pd.to_datetime(s, format=\"%Y-%m-%d\", errors=\"coerce\")\n",
    "    d2 = pd.to_datetime(s, format=\"%d/%m/%Y\", errors=\"coerce\")\n",
    "    d3 = pd.to_datetime(s, format=\"%m/%d/%Y\", errors=\"coerce\")\n",
    "    return d1.fillna(d2).fillna(d3)\n",
    "\n",
    "def read_csv_flex(path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(\n",
    "        path,\n",
    "        sep=None,\n",
    "        engine=\"python\",\n",
    "        encoding=\"utf-8-sig\",\n",
    "        dtype=str,\n",
    "        on_bad_lines=\"skip\",\n",
    "        quoting=3,\n",
    "        escapechar=\"\\\\\",\n",
    "        keep_default_na=False,\n",
    "    )\n",
    "    return df\n",
    "\n",
    "def save_csv_atomic(df: pd.DataFrame, path: str):\n",
    "    tmp = f\"{path}.tmp\"\n",
    "    df.to_csv(tmp, index=False)\n",
    "    os.replace(tmp, path)\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    retries = Retry(\n",
    "        total=3,\n",
    "        backoff_factor=0.6,\n",
    "        status_forcelist=[429, 500, 502, 503, 504],\n",
    "        allowed_methods=[\"GET\"],\n",
    "        raise_on_status=False,\n",
    "    )\n",
    "    s.headers.update(HEADERS)\n",
    "    s.mount(\"https://\", HTTPAdapter(max_retries=retries))\n",
    "    s.mount(\"http://\", HTTPAdapter(max_retries=retries))\n",
    "    return s\n",
    "\n",
    "def clean_num(x: str) -> str:\n",
    "    return x.replace(\"\\xa0\", \" \").replace(\" \", \"\").replace(\",\", \".\")\n",
    "\n",
    "# -----------------------------\n",
    "# Scraping avec pagination & cutoff\n",
    "# -----------------------------\n",
    "def scrape_ticker_since(ticker: str, last_date: dt.date, session: requests.Session) -> pd.DataFrame:\n",
    "    end_str = dt.date.today().strftime(\"%d-%m-%Y\")\n",
    "    collected = []\n",
    "\n",
    "    for page in range(1, MAX_PAGES + 1):\n",
    "        url = BASE_URL.format(ticker=ticker, end=end_str, page=page)\n",
    "        r = session.get(url, timeout=20)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"html.parser\")\n",
    "\n",
    "        table = soup.find(\"table\", {\"class\": \"table\"})\n",
    "        if table is None:\n",
    "            break\n",
    "        tbody = table.find(\"tbody\")\n",
    "        if tbody is None:\n",
    "            break\n",
    "\n",
    "        stop = False\n",
    "        for tr in tbody.find_all(\"tr\"):\n",
    "            tds = [td.get_text(strip=True) for td in tr.find_all(\"td\")]\n",
    "            if len(tds) < 7:\n",
    "                continue\n",
    "            try:\n",
    "                d = dt.datetime.strptime(tds[0], \"%d/%m/%Y\").date()\n",
    "            except ValueError:\n",
    "                continue\n",
    "\n",
    "            if last_date is not None and d <= last_date:\n",
    "                stop = True\n",
    "                break\n",
    "\n",
    "            collected.append({\n",
    "                \"Ticker\": f\"{ticker} J\",\n",
    "                \"Date\": d.strftime(\"%d/%m/%Y\"),\n",
    "                \"Variation\": clean_num(tds[1]).replace(\"%\", \"\"),\n",
    "                \"Volume Devise Total\": clean_num(tds[2]),\n",
    "                \"Cours Ajuste\": clean_num(tds[3]),\n",
    "                \"Volume Ajuste Total\": clean_num(tds[4]),\n",
    "                \"Cours Normal\": clean_num(tds[5]),\n",
    "                \"Volume Normal Total\": clean_num(tds[6]),\n",
    "                \"Date_dt\": pd.Timestamp(d),\n",
    "            })\n",
    "\n",
    "        if stop:\n",
    "            break\n",
    "        time.sleep(REQUEST_PAUSE)\n",
    "\n",
    "    if not collected:\n",
    "        return pd.DataFrame(columns=TARGET_COLS + [\"Date_dt\"])\n",
    "    return pd.DataFrame(collected)\n",
    "\n",
    "# -----------------------------\n",
    "# Update principal\n",
    "# -----------------------------\n",
    "def update_csv(csv_path: str = CSV_PATH):\n",
    "    if os.path.exists(csv_path):\n",
    "        base = read_csv_flex(csv_path)\n",
    "        for col in TARGET_COLS:\n",
    "            if col not in base.columns:\n",
    "                base[col] = pd.NA\n",
    "        base = base[TARGET_COLS]\n",
    "    else:\n",
    "        base = pd.DataFrame(columns=TARGET_COLS)\n",
    "\n",
    "    if not base.empty:\n",
    "        base[\"Ticker\"] = (\n",
    "            base[\"Ticker\"].astype(str)\n",
    "            .str.replace(r\"\\s+J$\", \"\", regex=True)\n",
    "            .str.strip() + \" J\"\n",
    "        )\n",
    "        base[\"Date\"] = base[\"Date\"].astype(str).map(_strip_all)\n",
    "\n",
    "    base[\"Date_dt\"] = parse_date_any(base[\"Date\"])\n",
    "\n",
    "    session = make_session()\n",
    "    updated_any = False\n",
    "\n",
    "    for ticker in TICKERS:\n",
    "        mask_t = base[\"Ticker\"] == f\"{ticker} J\"\n",
    "        last_date = None\n",
    "        if mask_t.any():\n",
    "            max_ts = base.loc[mask_t, \"Date_dt\"].max()\n",
    "            if pd.notna(max_ts):\n",
    "                last_date = max_ts.date()\n",
    "\n",
    "        df_new = scrape_ticker_since(ticker, last_date, session)\n",
    "\n",
    "        if not df_new.empty:\n",
    "            updated_any = True\n",
    "            base = pd.concat([base, df_new], ignore_index=True)\n",
    "            base = base.drop_duplicates(subset=[\"Ticker\", \"Date_dt\"])\n",
    "            print(f\"{len(df_new)} lignes ajoutées pour {ticker}\")\n",
    "        else:\n",
    "            print(f\"Aucune nouvelle ligne pour {ticker}\")\n",
    "\n",
    "        time.sleep(REQUEST_PAUSE)\n",
    "\n",
    "    # TRI final : par ticker puis date décroissante\n",
    "    base = base.sort_values([\"Ticker\", \"Date_dt\"], ascending=[True, False], na_position=\"last\")\n",
    "    base[\"Date\"] = base[\"Date_dt\"].dt.strftime(\"%d/%m/%Y\")\n",
    "    base = base[TARGET_COLS]\n",
    "\n",
    "    save_csv_atomic(base, csv_path)\n",
    "    if updated_any:\n",
    "        print(\"Table mise à jour :\", csv_path)\n",
    "    else:\n",
    "        print(\"table à jour (tri/format réappliqués)\")\n",
    "\n",
    "# -----------------------------\n",
    "# Entrypoint\n",
    "# -----------------------------\n",
    "if __name__ == \"__main__\":\n",
    "    update_csv(CSV_PATH)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
