{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5fec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üëâ Connecte-toi manuellement dans la fen√™tre qui s'ouvre...\n",
      "‚úÖ Cookies sauvegard√©s dans cookies.json\n"
     ]
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def save_cookies():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(\"https://www.richbourse.com/user/login\")\n",
    "\n",
    "        print(\"üëâ Connecte-toi manuellement dans la fen√™tre qui s'ouvre...\")\n",
    "        await page.wait_for_timeout(20000)  # 20 sec pour taper login/mdp\n",
    "\n",
    "        # Sauvegarde cookies + localStorage\n",
    "        await context.storage_state(path=\"cookies.json\")\n",
    "        await browser.close()\n",
    "        print(\"‚úÖ Cookies sauvegard√©s dans cookies.json\")\n",
    "\n",
    "import asyncio\n",
    "await save_cookies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä Scraping ABJC ...\n",
      "üìä Scraping BICB ...\n",
      "üìä Scraping BICC ...\n",
      "üìä Scraping BNBC ...\n",
      "üìä Scraping BOAB ...\n",
      "üìä Scraping BOABF ...\n",
      "üìä Scraping BOAC ...\n",
      "üìä Scraping BOAM ...\n",
      "üìä Scraping BOAN ...\n",
      "üìä Scraping BOAS ...\n",
      "üìä Scraping CABC ...\n",
      "üìä Scraping CBIBF ...\n",
      "üìä Scraping CFAC ...\n",
      "üìä Scraping CIEC ...\n",
      "üìä Scraping ECOC ...\n",
      "üìä Scraping ETIT ...\n",
      "üìä Scraping FTSC ...\n",
      "üìä Scraping LNBB ...\n",
      "üìä Scraping NEIC ...\n",
      "üìä Scraping NSBC ...\n",
      "üìä Scraping NTLC ...\n",
      "üìä Scraping ONTBF ...\n",
      "üìä Scraping ORAC ...\n",
      "üìä Scraping ORGT ...\n",
      "üìä Scraping PALC ...\n",
      "üìä Scraping PRSC ...\n",
      "üìä Scraping SAFC ...\n",
      "üìä Scraping SCRC ...\n",
      "üìä Scraping SDCC ...\n",
      "üìä Scraping SDSC ...\n",
      "üìä Scraping SEMC ...\n",
      "üìä Scraping SGBC ...\n",
      "üìä Scraping SHEC ...\n",
      "üìä Scraping SIBC ...\n",
      "üìä Scraping SICC ...\n",
      "üìä Scraping SIVC ...\n",
      "üìä Scraping SLBC ...\n",
      "üìä Scraping SMBC ...\n",
      "üìä Scraping SNTS ...\n",
      "üìä Scraping SOGC ...\n",
      "üìä Scraping SPHC ...\n",
      "üìä Scraping STAC ...\n",
      "üìä Scraping STBC ...\n",
      "üìä Scraping SVOC ...\n",
      "üìä Scraping TTLC ...\n",
      "üìä Scraping TTLS ...\n",
      "üìä Scraping UNLC ...\n",
      "üìä Scraping UNXC ...\n",
      "‚úÖ Fichier rapport_brvm_complet.csv g√©n√©r√©\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìà Dividendes ABJC ...\n",
      "üìà Dividendes BICB ...\n",
      "üìà Dividendes BICC ...\n",
      "üìà Dividendes BNBC ...\n",
      "üìà Dividendes BOAB ...\n",
      "üìà Dividendes BOABF ...\n",
      "üìà Dividendes BOAC ...\n",
      "üìà Dividendes BOAM ...\n",
      "üìà Dividendes BOAN ...\n",
      "üìà Dividendes BOAS ...\n",
      "üìà Dividendes CABC ...\n",
      "üìà Dividendes CBIBF ...\n",
      "üìà Dividendes CFAC ...\n",
      "üìà Dividendes CIEC ...\n",
      "üìà Dividendes ECOC ...\n",
      "üìà Dividendes ETIT ...\n",
      "üìà Dividendes FTSC ...\n",
      "üìà Dividendes LNBB ...\n",
      "üìà Dividendes NEIC ...\n",
      "üìà Dividendes NSBC ...\n",
      "üìà Dividendes NTLC ...\n",
      "üìà Dividendes ONTBF ...\n",
      "üìà Dividendes ORAC ...\n",
      "üìà Dividendes ORGT ...\n",
      "üìà Dividendes PALC ...\n",
      "üìà Dividendes PRSC ...\n",
      "üìà Dividendes SAFC ...\n",
      "üìà Dividendes SCRC ...\n",
      "üìà Dividendes SDCC ...\n",
      "üìà Dividendes SDSC ...\n",
      "üìà Dividendes SEMC ...\n",
      "üìà Dividendes SGBC ...\n",
      "üìà Dividendes SHEC ...\n",
      "üìà Dividendes SIBC ...\n",
      "üìà Dividendes SICC ...\n",
      "üìà Dividendes SIVC ...\n",
      "üìà Dividendes SLBC ...\n",
      "üìà Dividendes SMBC ...\n",
      "üìà Dividendes SNTS ...\n",
      "üìà Dividendes SOGC ...\n",
      "üìà Dividendes SPHC ...\n",
      "üìà Dividendes STAC ...\n",
      "üìà Dividendes STBC ...\n",
      "üìà Dividendes SVOC ...\n",
      "üìà Dividendes TTLC ...\n",
      "üìà Dividendes TTLS ...\n",
      "üìà Dividendes UNLC ...\n",
      "üìà Dividendes UNXC ...\n",
      "‚úÖ Fichier g√©n√©r√© : dividende_histo.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f60ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üîé ABJC\n",
      "üîé BICB\n",
      "üîé BICC\n",
      "üîé BNBC\n",
      "üîé BOAB\n",
      "üîé BOABF\n",
      "üîé BOAC\n",
      "üîé BOAM\n",
      "üîé BOAN\n",
      "üîé BOAS\n",
      "üîé CABC\n",
      "üîé CBIBF\n",
      "üîé CFAC\n",
      "üîé CIEC\n",
      "üîé ECOC\n",
      "üîé ETIT\n",
      "üîé FTSC\n",
      "üîé LNBB\n",
      "üîé NEIC\n",
      "üîé NSBC\n",
      "üîé NTLC\n",
      "üîé ONTBF\n",
      "üîé ORAC\n",
      "üîé ORGT\n",
      "üîé PALC\n",
      "üîé PRSC\n",
      "üîé SAFC\n",
      "üîé SCRC\n",
      "üîé SDCC\n",
      "üîé SDSC\n",
      "üîé SEMC\n",
      "üîé SGBC\n",
      "üîé SHEC\n",
      "üîé SIBC\n",
      "üîé SICC\n",
      "üîé SIVC\n",
      "üîé SLBC\n",
      "üîé SMBC\n",
      "üîé SNTS\n",
      "üîé SOGC\n",
      "üîé SPHC\n",
      "üîé STAC\n",
      "üîé STBC\n",
      "üîé SVOC\n",
      "üîé TTLC\n",
      "üîé TTLS\n",
      "üîé UNLC\n",
      "üîé UNXC\n",
      "‚úÖ Fichier g√©n√©r√© : ratios.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb81a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìä ABJC ...\n",
      "üìä BICB ...\n",
      "üìä BICC ...\n",
      "üìä BNBC ...\n",
      "üìä BOAB ...\n",
      "üìä BOABF ...\n",
      "üìä BOAC ...\n",
      "üìä BOAM ...\n",
      "üìä BOAN ...\n",
      "üìä BOAS ...\n",
      "üìä CABC ...\n",
      "üìä CBIBF ...\n",
      "üìä CFAC ...\n",
      "üìä CIEC ...\n",
      "üìä ECOC ...\n",
      "üìä ETIT ...\n",
      "üìä FTSC ...\n",
      "üìä LNBB ...\n",
      "üìä NEIC ...\n",
      "üìä NSBC ...\n",
      "üìä NTLC ...\n",
      "üìä ONTBF ...\n",
      "üìä ORAC ...\n",
      "üìä ORGT ...\n",
      "üìä PALC ...\n",
      "üìä PRSC ...\n",
      "üìä SAFC ...\n",
      "üìä SCRC ...\n",
      "üìä SDCC ...\n",
      "üìä SDSC ...\n",
      "üìä SEMC ...\n",
      "üìä SGBC ...\n",
      "üìä SHEC ...\n",
      "üìä SIBC ...\n",
      "üìä SICC ...\n",
      "üìä SIVC ...\n",
      "üìä SLBC ...\n",
      "üìä SMBC ...\n",
      "üìä SNTS ...\n",
      "üìä SOGC ...\n",
      "üìä SPHC ...\n",
      "üìä STAC ...\n",
      "üìä STBC ...\n",
      "üìä SVOC ...\n",
      "üìä TTLC ...\n",
      "üìä TTLS ...\n",
      "üìä UNLC ...\n",
      "üìä UNXC ...\n",
      "‚úÖ Fichier g√©n√©r√© : valorisation_performances.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440dcd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üìò ABJC\n",
      "üìò BICB\n",
      "üìò BICC\n",
      "üìò BNBC\n",
      "üìò BOAB\n",
      "üìò BOABF\n",
      "üìò BOAC\n",
      "üìò BOAM\n",
      "üìò BOAN\n",
      "üìò BOAS\n",
      "üìò CABC\n",
      "üìò CBIBF\n",
      "üìò CFAC\n",
      "üìò CIEC\n",
      "üìò ECOC\n",
      "üìò ETIT\n",
      "üìò FTSC\n",
      "üìò LNBB\n",
      "üìò NEIC\n",
      "üìò NSBC\n",
      "üìò NTLC\n",
      "üìò ONTBF\n",
      "üìò ORAC\n",
      "üìò ORGT\n",
      "üìò PALC\n",
      "üìò PRSC\n",
      "üìò SAFC\n",
      "üìò SCRC\n",
      "üìò SDCC\n",
      "üìò SDSC\n",
      "üìò SEMC\n",
      "üìò SGBC\n",
      "üìò SHEC\n",
      "üìò SIBC\n",
      "üìò SICC\n",
      "üìò SIVC\n",
      "üìò SLBC\n",
      "üìò SMBC\n",
      "üìò SNTS\n",
      "üìò SOGC\n",
      "üìò SPHC\n",
      "üìò STAC\n",
      "üìò STBC\n",
      "üìò SVOC\n",
      "üìò TTLC\n",
      "üìò TTLS\n",
      "üìò UNLC\n",
      "üìò UNXC\n",
      "‚úÖ societes_details.csv √©crit\n",
      "‚ÑπÔ∏è Aucun fractionnement trouv√©\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a08db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------------------- R√©glages --------------------\n",
    "BASE = \"https://www.richbourse.com\"\n",
    "LIST_TPL = BASE + \"/common/variation/index/veille/tout?page={}\"  # pagination ?page=1,2,3...\n",
    "UA = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6) AppleWebKit/537.36 \"\n",
    "      \"(KHTML, like Gecko) Chrome/126.0 Safari/537.36\")\n",
    "MAX_PAGES = 500      # garde-fou\n",
    "SLEEP_SEC = 0.25     # petite pause entre pages\n",
    "\n",
    "# -------------------- Utils parsing --------------------\n",
    "def norm(s: str) -> str:\n",
    "    \"\"\"Nettoyage l√©ger du texte extrait du HTML.\"\"\"\n",
    "    return (s or \"\").replace(\"\\xa0\", \" \").replace(\"\\u200b\", \"\").strip()\n",
    "\n",
    "def parse_table_html(html: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retourne un DataFrame √† partir du 1er <table> (thead facultatif).\n",
    "    Harmonise automatiquement les en-t√™tes si absents.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\") or soup.find(\"table\", {\"class\": \"table\"})\n",
    "    if table is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Ent√™tes\n",
    "    headers = []\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [norm(th.get_text(strip=True)) for th in thead.find_all(\"th\")]\n",
    "\n",
    "    # Lignes\n",
    "    tbody = table.find(\"tbody\")\n",
    "    if tbody is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for tr in tbody.find_all(\"tr\"):\n",
    "        tds = tr.find_all([\"td\", \"th\"])\n",
    "        if not tds:\n",
    "            continue\n",
    "        cells = [norm(td.get_text(\" \", strip=True)) for td in tds]\n",
    "        if any(cells):\n",
    "            rows.append(cells)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Si pas d'en-t√™tes -> prendre la 1re ligne comme ent√™tes\n",
    "    if not headers:\n",
    "        headers = [norm(c) for c in rows[0]]\n",
    "        rows = rows[1:]\n",
    "\n",
    "    # Harmoniser largeurs\n",
    "    width = min(len(headers), max((len(r) for r in rows), default=0))\n",
    "    if width == 0:\n",
    "        return pd.DataFrame()\n",
    "    headers = headers[:width]\n",
    "    rows = [r[:width] for r in rows]\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "    except Exception:\n",
    "        # fallback si tailles incoh√©rentes\n",
    "        df = pd.DataFrame(rows)\n",
    "        if len(df.columns) == len(headers):\n",
    "            df.columns = headers\n",
    "    return df\n",
    "\n",
    "async def goto_and_wait(page, url: str):\n",
    "    \"\"\"Navigation + attentes raisonnables pour contenu du tableau.\"\"\"\n",
    "    await page.goto(url, timeout=60000)\n",
    "    await page.wait_for_load_state(\"domcontentloaded\")\n",
    "    try:\n",
    "        await page.wait_for_selector(\"table\", timeout=10000)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        await page.wait_for_load_state(\"networkidle\", timeout=5000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# -------------------- Scraper principal --------------------\n",
    "async def scrape_veille_all(headless: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parcourt toutes les pages du tableau 'Veille' et renvoie un DataFrame consolid√©.\n",
    "    \"\"\"\n",
    "    out_frames = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(\n",
    "            headless=headless,\n",
    "            args=[\"--disable-blink-features=AutomationControlled\"]\n",
    "        )\n",
    "        context = await browser.new_context(user_agent=UA, locale=\"fr-FR\")\n",
    "        page = await context.new_page()\n",
    "\n",
    "        page_no = 1\n",
    "        while page_no <= MAX_PAGES:\n",
    "            url = LIST_TPL.format(page_no)\n",
    "            print(f\"üìÑ Page {page_no} ‚Üí {url}\")\n",
    "            await goto_and_wait(page, url)\n",
    "\n",
    "            # petit check de redirection improbable\n",
    "            if \"/user/login\" in page.url or \"/investisseur/profile\" in page.url:\n",
    "                await goto_and_wait(page, url)\n",
    "\n",
    "            try:\n",
    "                html = await page.inner_html(\"table\", timeout=8000)\n",
    "            except:\n",
    "                print(\"  ‚ö†Ô∏è pas de <table> visible ‚Äî fin.\")\n",
    "                break\n",
    "\n",
    "            df = parse_table_html(html)\n",
    "            if df.empty:\n",
    "                print(\"  ‚ÑπÔ∏è aucune ligne ‚Äî fin de pagination.\")\n",
    "                break\n",
    "\n",
    "            # Ajout d‚Äôun marqueur de page (optionnel)\n",
    "            df[\"__page__\"] = page_no\n",
    "            out_frames.append(df)\n",
    "\n",
    "            # Heuristique de fin : tr√®s peu de lignes => probablement derni√®re page\n",
    "            if len(df) < 5:\n",
    "                print(\"  ‚ÑπÔ∏è peu de lignes ‚Äî fin probable.\")\n",
    "                break\n",
    "\n",
    "            page_no += 1\n",
    "            time.sleep(SLEEP_SEC)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    if not out_frames:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    full = pd.concat(out_frames, ignore_index=True)\n",
    "\n",
    "    # Optionnel : supprimer les colonnes vides int√©gralement\n",
    "    empty_cols = [c for c in full.columns if full[c].astype(str).str.strip().eq(\"\").all()]\n",
    "    full = full.drop(columns=empty_cols, errors=\"ignore\")\n",
    "\n",
    "    return full\n",
    "\n",
    "# -------------------- Ex√©cution --------------------\n",
    "# Choisissez UNE des deux options ci-dessous :\n",
    "\n",
    "# 1) >>> Dans Jupyter/Notebook :\n",
    "# df = await scrape_veille_all(headless=False)\n",
    "# df.to_csv(\"veille.csv\", index=False)\n",
    "# print(f\"‚úÖ OK : {len(df)} lignes ‚Üí veille.csv\")\n",
    "# display(df.head())\n",
    "\n",
    "# 2) >>> En script .py (terminal) :\n",
    "# if __name__ == \"__main__\":\n",
    "#     import asyncio\n",
    "#     df = asyncio.run(scrape_veille_all(headless=True))\n",
    "#     df.to_csv(\"veille.csv\", index=False)\n",
    "#     print(f\"‚úÖ OK : {len(df)} lignes ‚Üí veille.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f094e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/48] ABJC ...\n",
      "[2/48] BICB ...\n",
      "[3/48] BICC ...\n",
      "[4/48] BNBC ...\n",
      "[5/48] BOAB ...\n",
      "[6/48] BOABF ...\n",
      "[7/48] BOAC ...\n",
      "[8/48] BOAM ...\n",
      "[9/48] BOAN ...\n",
      "[10/48] BOAS ...\n",
      "[11/48] CABC ...\n",
      "[12/48] CBIBF ...\n",
      "[13/48] CFAC ...\n",
      "[14/48] CIEC ...\n",
      "[15/48] ECOC ...\n",
      "[16/48] ETIT ...\n",
      "[17/48] FTSC ...\n",
      "[18/48] LNBB ...\n",
      "[19/48] NEIC ...\n",
      "[20/48] NSBC ...\n",
      "[21/48] NTLC ...\n",
      "[22/48] ONTBF ...\n",
      "[23/48] ORAC ...\n",
      "[24/48] ORGT ...\n",
      "[25/48] PALC ...\n",
      "[26/48] PRSC ...\n",
      "[27/48] SAFC ...\n",
      "[28/48] SCRC ...\n",
      "[29/48] SDCC ...\n",
      "[30/48] SDSC ...\n",
      "[31/48] SEMC ...\n",
      "[32/48] SGBC ...\n",
      "[33/48] SHEC ...\n",
      "[34/48] SIBC ...\n",
      "[35/48] SICC ...\n",
      "[36/48] SIVC ...\n",
      "[37/48] SLBC ...\n",
      "[38/48] SMBC ...\n",
      "[39/48] SNTS ...\n",
      "[40/48] SOGC ...\n",
      "[41/48] SPHC ...\n",
      "[42/48] STAC ...\n",
      "[43/48] STBC ...\n",
      "[44/48] SVOC ...\n",
      "[45/48] TTLC ...\n",
      "[46/48] TTLS ...\n",
      "[47/48] UNLC ...\n",
      "[48/48] UNXC ...\n",
      "\n",
      "Termin√©.\n",
      "CSV : richbourse_societes.csv\n",
      "XLSX : richbourse_societes.xlsx\n",
      "Logos : /Users/jeanjoelgoli/Documents/FINANCE/Travaux BRVM/logos_richbourse\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2537159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/48] ABJC ‚Ä¶\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/svq7n_mn2lg1fv5jbk66hzg00000gn/T/ipykernel_12105/1316626944.py:176: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"scraped_at\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/48] BICB ‚Ä¶\n",
      "[3/48] BICC ‚Ä¶\n",
      "[4/48] BNBC ‚Ä¶\n",
      "[5/48] BOAB ‚Ä¶\n",
      "[6/48] BOABF ‚Ä¶\n",
      "[7/48] BOAC ‚Ä¶\n",
      "[8/48] BOAM ‚Ä¶\n",
      "[9/48] BOAN ‚Ä¶\n",
      "[10/48] BOAS ‚Ä¶\n",
      "[11/48] CABC ‚Ä¶\n",
      "[12/48] CBIBF ‚Ä¶\n",
      "[13/48] CFAC ‚Ä¶\n",
      "[14/48] CIEC ‚Ä¶\n",
      "[15/48] ECOC ‚Ä¶\n",
      "[16/48] ETIT ‚Ä¶\n",
      "[17/48] FTSC ‚Ä¶\n",
      "[18/48] LNBB ‚Ä¶\n",
      "[19/48] NEIC ‚Ä¶\n",
      "[20/48] NSBC ‚Ä¶\n",
      "[21/48] NTLC ‚Ä¶\n",
      "[22/48] ONTBF ‚Ä¶\n",
      "[23/48] ORAC ‚Ä¶\n",
      "[24/48] ORGT ‚Ä¶\n",
      "[25/48] PALC ‚Ä¶\n",
      "[26/48] PRSC ‚Ä¶\n",
      "[27/48] SAFC ‚Ä¶\n",
      "[28/48] SCRC ‚Ä¶\n",
      "[29/48] SDCC ‚Ä¶\n",
      "[30/48] SDSC ‚Ä¶\n",
      "[31/48] SEMC ‚Ä¶\n",
      "[32/48] SGBC ‚Ä¶\n",
      "[33/48] SHEC ‚Ä¶\n",
      "[34/48] SIBC ‚Ä¶\n",
      "[35/48] SICC ‚Ä¶\n",
      "[36/48] SIVC ‚Ä¶\n",
      "[37/48] SLBC ‚Ä¶\n",
      "[38/48] SMBC ‚Ä¶\n",
      "[39/48] SNTS ‚Ä¶\n",
      "[40/48] SOGC ‚Ä¶\n",
      "[41/48] SPHC ‚Ä¶\n",
      "[42/48] STAC ‚Ä¶\n",
      "[43/48] STBC ‚Ä¶\n",
      "[44/48] SVOC ‚Ä¶\n",
      "[45/48] TTLC ‚Ä¶\n",
      "[46/48] TTLS ‚Ä¶\n",
      "[47/48] UNLC ‚Ä¶\n",
      "[48/48] UNXC ‚Ä¶\n",
      "\n",
      "OK. CSV: richbourse_mouvements.csv | XLSX: richbourse_mouvements.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "TICKERS = [\n",
    "    \"ABJC\",\"BICB\",\"BICC\",\"BNBC\",\"BOAB\",\"BOABF\",\"BOAC\",\"BOAM\",\"BOAN\",\"BOAS\",\n",
    "    \"CABC\",\"CBIBF\",\"CFAC\",\"CIEC\",\"ECOC\",\"ETIT\",\"FTSC\",\"LNBB\",\"NEIC\",\"NSBC\",\n",
    "    \"NTLC\",\"ONTBF\",\"ORAC\",\"ORGT\",\"PALC\",\"PRSC\",\"SAFC\",\"SCRC\",\"SDCC\",\"SDSC\",\n",
    "    \"SEMC\",\"SGBC\",\"SHEC\",\"SIBC\",\"SICC\",\"SIVC\",\"SLBC\",\"SMBC\",\"SNTS\",\"SOGC\",\n",
    "    \"SPHC\",\"STAC\",\"STBC\",\"SVOC\",\"TTLC\",\"TTLS\",\"UNLC\",\"UNXC\"\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://www.richbourse.com/common/mouvements/index/{ticker}\"\n",
    "OUT_CSV  = \"richbourse_mouvements.csv\"\n",
    "OUT_XLSX = \"richbourse_mouvements.xlsx\"\n",
    "\n",
    "# Mapping canonique -> variantes possibles qu'on peut voir √† gauche\n",
    "FIELD_MAP = {\n",
    "    \"Prix (FCFA)\": [r\"^\\s*\\d[\\d\\s.,]*\\s*FCFA\\s*$\"],  # gros prix en haut (on le traite √† part)\n",
    "    \"Variation (%)\": [r\"%\"],                         # idem (on le traite √† part)\n",
    "    \"Volume (titres)\": [r\"^Volume\\s*\\(titres\\)\\s*$\"],\n",
    "    \"Valeur (FCFA)\": [r\"^Valeur\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Ouverture (FCFA)\": [r\"^Ouverture\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Plus haut (FCFA)\": [r\"^Plus haut\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Plus bas (FCFA)\": [r\"^Plus bas\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Cl√¥ture jour (FCFA)\": [r\"^Cl√¥ture jour\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Cl√¥ture veille (FCFA)\": [r\"^Cl√¥ture veille\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Nombre total de titres\": [r\"^Nombre total de titres\\s*$\"],\n",
    "    \"Titres du flottant\": [r\"^Titres du flottant\\s*$\"],\n",
    "    \"Volume moyen\": [r\"^Volume moyen\\s*$\"],\n",
    "    \"Valorisation (MFCFA)\": [r\"^Valorisation\\s*\\(MFCFA\\)\\s*$\"],\n",
    "}\n",
    "\n",
    "CANON_KEYS = list(FIELD_MAP.keys())\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/126.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.9\",\n",
    "    })\n",
    "    s.mount(\"https://\", requests.adapters.HTTPAdapter(max_retries=3))\n",
    "    s.mount(\"http://\", requests.adapters.HTTPAdapter(max_retries=3))\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "num_re = re.compile(r\"[-+]?\\d[\\d\\s.,]*\")\n",
    "\n",
    "def to_float(s: Optional[str]) -> Optional[float]:\n",
    "    if not s:\n",
    "        return None\n",
    "    s = str(s)\n",
    "    m = num_re.search(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    n = m.group(0).replace(\" \", \"\").replace(\"\\xa0\", \"\").replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(n)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def to_int(s: Optional[str]) -> Optional[int]:\n",
    "    f = to_float(s)\n",
    "    return int(round(f)) if f is not None else None\n",
    "\n",
    "def guess_left_panel(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Essaie de cibler la colonne/panneau de gauche.\n",
    "    Heuristique: bloc qui contient les libell√©s 'Volume (titres)', 'Ouverture (FCFA)', etc.\n",
    "    \"\"\"\n",
    "    candidates = soup.find_all(True, recursive=True)\n",
    "    best = None\n",
    "    score_best = -1\n",
    "    wanted_labels = [\"Volume (titres)\", \"Ouverture (FCFA)\", \"Cl√¥ture veille (FCFA)\"]\n",
    "    for tag in candidates:\n",
    "        txt = \" \".join(tag.stripped_strings)\n",
    "        score = sum(1 for w in wanted_labels if w in txt)\n",
    "        if score > score_best:\n",
    "            score_best = score\n",
    "            best = tag\n",
    "    return best or soup\n",
    "\n",
    "def extract_price_and_change(soup_left) -> (Optional[float], Optional[float]):\n",
    "    \"\"\"\n",
    "    R√©cup√®re le gros prix en FCFA et la variation en % dans le bandeau vert (ou voisin).\n",
    "    Heuristique: chercher une ligne avec 'FCFA' puis √† proximit√© '%'.\n",
    "    \"\"\"\n",
    "    text = \" \".join(soup_left.stripped_strings)\n",
    "    # prix FCFA (le plus grand premier match)\n",
    "    price = None\n",
    "    for m in re.finditer(r\"\\d[\\d\\s.,]*\\s*FCFA\", text):\n",
    "        price_str = m.group(0)\n",
    "        price = to_float(price_str)\n",
    "        if price is not None:\n",
    "            break\n",
    "    # variation %\n",
    "    change = None\n",
    "    m = re.search(r\"[-+]?\\d[\\d\\s.,]*\\s*%\", text)\n",
    "    if m:\n",
    "        change = to_float(m.group(0))\n",
    "    return price, change\n",
    "\n",
    "def extract_kv_pairs(soup_left) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parcourt le bloc pour trouver des paires Libell√© -> Valeur.\n",
    "    Beaucoup de pages utilisent des <tr><td> ou des <div> en deux colonnes.\n",
    "    On d√©tecte les libell√©s connus via FIELD_MAP, puis on prend la valeur voisine.\n",
    "    \"\"\"\n",
    "    results: Dict[str, str] = {}\n",
    "    # Strat√©gie 1: chercher des TR / TD\n",
    "    for tr in soup_left.find_all(\"tr\"):\n",
    "        tds = [normalize(\" \".join(td.stripped_strings)) for td in tr.find_all(\"td\")]\n",
    "        if len(tds) >= 2:\n",
    "            label, value = tds[0], tds[1]\n",
    "            canon = canonize_label(label)\n",
    "            if canon:\n",
    "                results[canon] = value\n",
    "    # Strat√©gie 2: blocs <div> \"ligne\"\n",
    "    # On ne remplace pas ce qui a d√©j√† √©t√© trouv√©\n",
    "    divs = soup_left.find_all(\"div\")\n",
    "    for i, div in enumerate(divs):\n",
    "        label = normalize(div.get_text(\" \", strip=True))\n",
    "        canon = canonize_label(label)\n",
    "        if canon and canon not in results:\n",
    "            # valeur: essayer fr√®re suivant, ou texte du parent imm√©diat\n",
    "            value = None\n",
    "            sib = div.find_next_sibling()\n",
    "            if sib:\n",
    "                value = normalize(sib.get_text(\" \", strip=True))\n",
    "            if not value:\n",
    "                parent = div.parent\n",
    "                if parent:\n",
    "                    txts = [normalize(x) for x in parent.stripped_strings]\n",
    "                    if len(txts) >= 2 and normalize(txts[0]) == label:\n",
    "                        value = txts[1]\n",
    "            if value:\n",
    "                results[canon] = value\n",
    "    # Strat√©gie 3: lecture lin√©aire par proximit√© (fallback)\n",
    "    if len(results) < 5:\n",
    "        items = [normalize(s) for s in soup_left.stripped_strings]\n",
    "        for j, token in enumerate(items[:-1]):\n",
    "            canon = canonize_label(token)\n",
    "            if canon and canon not in results:\n",
    "                results[canon] = items[j+1]\n",
    "    return results\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def canonize_label(label: str) -> Optional[str]:\n",
    "    lab = normalize(label)\n",
    "    for canon, patterns in FIELD_MAP.items():\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, lab, flags=re.IGNORECASE):\n",
    "                return canon\n",
    "    return None\n",
    "\n",
    "def scrape_one(ticker: str) -> Dict[str, Optional[str]]:\n",
    "    url = BASE_URL.format(ticker=ticker)\n",
    "    row: Dict[str, Optional[str]] = {\n",
    "        \"ticker\": ticker,\n",
    "        \"source_url\": url,\n",
    "        \"scraped_at\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        # valeurs brutes\n",
    "        \"Prix (FCFA)\": None,\n",
    "        \"Variation (%)\": None,\n",
    "        \"Volume (titres)\": None,\n",
    "        \"Valeur (FCFA)\": None,\n",
    "        \"Ouverture (FCFA)\": None,\n",
    "        \"Plus haut (FCFA)\": None,\n",
    "        \"Plus bas (FCFA)\": None,\n",
    "        \"Cl√¥ture jour (FCFA)\": None,\n",
    "        \"Cl√¥ture veille (FCFA)\": None,\n",
    "        \"Nombre total de titres\": None,\n",
    "        \"Titres du flottant\": None,\n",
    "        \"Volume moyen\": None,\n",
    "        \"Valorisation (MFCFA)\": None,\n",
    "        # versions num√©riques utiles\n",
    "        \"prix_fcfa\": None,\n",
    "        \"variation_pct\": None,\n",
    "        \"volume_titres\": None,\n",
    "        \"valeur_fcfa\": None,\n",
    "        \"ouverture_fcfa\": None,\n",
    "        \"plus_haut_fcfa\": None,\n",
    "        \"plus_bas_fcfa\": None,\n",
    "        \"cloture_jour_fcfa\": None,\n",
    "        \"cloture_veille_fcfa\": None,\n",
    "        \"nb_total_titres\": None,\n",
    "        \"titres_flottant\": None,\n",
    "        \"volume_moyen\": None,\n",
    "        \"valorisation_mfcfa\": None,\n",
    "        \"valorisation_fcfa\": None,\n",
    "    }\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=25)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        left = guess_left_panel(soup)\n",
    "\n",
    "        # prix + variation\n",
    "        price, change = extract_price_and_change(left)\n",
    "        if price is not None:\n",
    "            row[\"Prix (FCFA)\"] = f\"{price:.0f} FCFA\"\n",
    "            row[\"prix_fcfa\"] = price\n",
    "        if change is not None:\n",
    "            row[\"Variation (%)\"] = f\"{change:.2f}%\"\n",
    "            row[\"variation_pct\"] = change\n",
    "\n",
    "        # paires label/valeur\n",
    "        kv = extract_kv_pairs(left)\n",
    "        for k in CANON_KEYS:\n",
    "            if k in kv:\n",
    "                row[k] = kv[k]\n",
    "\n",
    "        # cast num√©riques\n",
    "        row[\"volume_titres\"] = to_int(row[\"Volume (titres)\"])\n",
    "        row[\"valeur_fcfa\"] = to_float(row[\"Valeur (FCFA)\"])\n",
    "        row[\"ouverture_fcfa\"] = to_float(row[\"Ouverture (FCFA)\"])\n",
    "        row[\"plus_haut_fcfa\"] = to_float(row[\"Plus haut (FCFA)\"])\n",
    "        row[\"plus_bas_fcfa\"] = to_float(row[\"Plus bas (FCFA)\"])\n",
    "        row[\"cloture_jour_fcfa\"] = to_float(row[\"Cl√¥ture jour (FCFA)\"])\n",
    "        row[\"cloture_veille_fcfa\"] = to_float(row[\"Cl√¥ture veille (FCFA)\"])\n",
    "        row[\"nb_total_titres\"] = to_int(row[\"Nombre total de titres\"])\n",
    "        row[\"titres_flottant\"] = to_int(row[\"Titres du flottant\"])\n",
    "        row[\"volume_moyen\"] = to_int(row[\"Volume moyen\"])\n",
    "        row[\"valorisation_mfcfa\"] = to_float(row[\"Valorisation (MFCFA)\"])\n",
    "        if row[\"valorisation_mfcfa\"] is not None:\n",
    "            row[\"valorisation_fcfa\"] = row[\"valorisation_mfcfa\"] * 1_000_000.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] ERREUR: {e}\")\n",
    "\n",
    "    return row\n",
    "\n",
    "def scrape_all(tickers: List[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, t in enumerate(tickers, 1):\n",
    "        print(f\"[{i}/{len(tickers)}] {t} ‚Ä¶\")\n",
    "        rows.append(scrape_one(t))\n",
    "        time.sleep(0.6)  # politesse\n",
    "    cols_order = [\n",
    "        \"ticker\",\"source_url\",\"scraped_at\",\n",
    "        \"Prix (FCFA)\",\"Variation (%)\",\n",
    "        \"Volume (titres)\",\"Valeur (FCFA)\",\"Ouverture (FCFA)\",\n",
    "        \"Plus haut (FCFA)\",\"Plus bas (FCFA)\",\n",
    "        \"Cl√¥ture jour (FCFA)\",\"Cl√¥ture veille (FCFA)\",\n",
    "        \"Nombre total de titres\",\"Titres du flottant\",\n",
    "        \"Volume moyen\",\"Valorisation (MFCFA)\",\n",
    "        \"prix_fcfa\",\"variation_pct\",\"volume_titres\",\"valeur_fcfa\",\n",
    "        \"ouverture_fcfa\",\"plus_haut_fcfa\",\"plus_bas_fcfa\",\n",
    "        \"cloture_jour_fcfa\",\"cloture_veille_fcfa\",\n",
    "        \"nb_total_titres\",\"titres_flottant\",\"volume_moyen\",\n",
    "        \"valorisation_mfcfa\",\"valorisation_fcfa\"\n",
    "    ]\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df[cols_order]\n",
    "\n",
    "def main():\n",
    "    df = scrape_all(TICKERS)\n",
    "    df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    try:\n",
    "        df.to_excel(OUT_XLSX, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Impossible d‚Äô√©crire l‚ÄôXLSX (openpyxl manquant ?): {e}\")\n",
    "    print(f\"\\nOK. CSV: {OUT_CSV} | XLSX: {OUT_XLSX}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
