{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2f5fec57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "👉 Connecte-toi manuellement dans la fenêtre qui s'ouvre...\n",
      "✅ Cookies sauvegardés dans cookies.json\n"
     ]
    }
   ],
   "source": [
    "from playwright.async_api import async_playwright\n",
    "\n",
    "async def save_cookies():\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(headless=False)\n",
    "        context = await browser.new_context()\n",
    "        page = await context.new_page()\n",
    "        await page.goto(\"https://www.richbourse.com/user/login\")\n",
    "\n",
    "        print(\"👉 Connecte-toi manuellement dans la fenêtre qui s'ouvre...\")\n",
    "        await page.wait_for_timeout(20000)  # 20 sec pour taper login/mdp\n",
    "\n",
    "        # Sauvegarde cookies + localStorage\n",
    "        await context.storage_state(path=\"cookies.json\")\n",
    "        await browser.close()\n",
    "        print(\"✅ Cookies sauvegardés dans cookies.json\")\n",
    "\n",
    "import asyncio\n",
    "await save_cookies()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6cf6499",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 Scraping ABJC ...\n",
      "📊 Scraping BICB ...\n",
      "📊 Scraping BICC ...\n",
      "📊 Scraping BNBC ...\n",
      "📊 Scraping BOAB ...\n",
      "📊 Scraping BOABF ...\n",
      "📊 Scraping BOAC ...\n",
      "📊 Scraping BOAM ...\n",
      "📊 Scraping BOAN ...\n",
      "📊 Scraping BOAS ...\n",
      "📊 Scraping CABC ...\n",
      "📊 Scraping CBIBF ...\n",
      "📊 Scraping CFAC ...\n",
      "📊 Scraping CIEC ...\n",
      "📊 Scraping ECOC ...\n",
      "📊 Scraping ETIT ...\n",
      "📊 Scraping FTSC ...\n",
      "📊 Scraping LNBB ...\n",
      "📊 Scraping NEIC ...\n",
      "📊 Scraping NSBC ...\n",
      "📊 Scraping NTLC ...\n",
      "📊 Scraping ONTBF ...\n",
      "📊 Scraping ORAC ...\n",
      "📊 Scraping ORGT ...\n",
      "📊 Scraping PALC ...\n",
      "📊 Scraping PRSC ...\n",
      "📊 Scraping SAFC ...\n",
      "📊 Scraping SCRC ...\n",
      "📊 Scraping SDCC ...\n",
      "📊 Scraping SDSC ...\n",
      "📊 Scraping SEMC ...\n",
      "📊 Scraping SGBC ...\n",
      "📊 Scraping SHEC ...\n",
      "📊 Scraping SIBC ...\n",
      "📊 Scraping SICC ...\n",
      "📊 Scraping SIVC ...\n",
      "📊 Scraping SLBC ...\n",
      "📊 Scraping SMBC ...\n",
      "📊 Scraping SNTS ...\n",
      "📊 Scraping SOGC ...\n",
      "📊 Scraping SPHC ...\n",
      "📊 Scraping STAC ...\n",
      "📊 Scraping STBC ...\n",
      "📊 Scraping SVOC ...\n",
      "📊 Scraping TTLC ...\n",
      "📊 Scraping TTLS ...\n",
      "📊 Scraping UNLC ...\n",
      "📊 Scraping UNXC ...\n",
      "✅ Fichier rapport_brvm_complet.csv généré\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f0400c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📈 Dividendes ABJC ...\n",
      "📈 Dividendes BICB ...\n",
      "📈 Dividendes BICC ...\n",
      "📈 Dividendes BNBC ...\n",
      "📈 Dividendes BOAB ...\n",
      "📈 Dividendes BOABF ...\n",
      "📈 Dividendes BOAC ...\n",
      "📈 Dividendes BOAM ...\n",
      "📈 Dividendes BOAN ...\n",
      "📈 Dividendes BOAS ...\n",
      "📈 Dividendes CABC ...\n",
      "📈 Dividendes CBIBF ...\n",
      "📈 Dividendes CFAC ...\n",
      "📈 Dividendes CIEC ...\n",
      "📈 Dividendes ECOC ...\n",
      "📈 Dividendes ETIT ...\n",
      "📈 Dividendes FTSC ...\n",
      "📈 Dividendes LNBB ...\n",
      "📈 Dividendes NEIC ...\n",
      "📈 Dividendes NSBC ...\n",
      "📈 Dividendes NTLC ...\n",
      "📈 Dividendes ONTBF ...\n",
      "📈 Dividendes ORAC ...\n",
      "📈 Dividendes ORGT ...\n",
      "📈 Dividendes PALC ...\n",
      "📈 Dividendes PRSC ...\n",
      "📈 Dividendes SAFC ...\n",
      "📈 Dividendes SCRC ...\n",
      "📈 Dividendes SDCC ...\n",
      "📈 Dividendes SDSC ...\n",
      "📈 Dividendes SEMC ...\n",
      "📈 Dividendes SGBC ...\n",
      "📈 Dividendes SHEC ...\n",
      "📈 Dividendes SIBC ...\n",
      "📈 Dividendes SICC ...\n",
      "📈 Dividendes SIVC ...\n",
      "📈 Dividendes SLBC ...\n",
      "📈 Dividendes SMBC ...\n",
      "📈 Dividendes SNTS ...\n",
      "📈 Dividendes SOGC ...\n",
      "📈 Dividendes SPHC ...\n",
      "📈 Dividendes STAC ...\n",
      "📈 Dividendes STBC ...\n",
      "📈 Dividendes SVOC ...\n",
      "📈 Dividendes TTLC ...\n",
      "📈 Dividendes TTLS ...\n",
      "📈 Dividendes UNLC ...\n",
      "📈 Dividendes UNXC ...\n",
      "✅ Fichier généré : dividende_histo.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f60ab3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔎 ABJC\n",
      "🔎 BICB\n",
      "🔎 BICC\n",
      "🔎 BNBC\n",
      "🔎 BOAB\n",
      "🔎 BOABF\n",
      "🔎 BOAC\n",
      "🔎 BOAM\n",
      "🔎 BOAN\n",
      "🔎 BOAS\n",
      "🔎 CABC\n",
      "🔎 CBIBF\n",
      "🔎 CFAC\n",
      "🔎 CIEC\n",
      "🔎 ECOC\n",
      "🔎 ETIT\n",
      "🔎 FTSC\n",
      "🔎 LNBB\n",
      "🔎 NEIC\n",
      "🔎 NSBC\n",
      "🔎 NTLC\n",
      "🔎 ONTBF\n",
      "🔎 ORAC\n",
      "🔎 ORGT\n",
      "🔎 PALC\n",
      "🔎 PRSC\n",
      "🔎 SAFC\n",
      "🔎 SCRC\n",
      "🔎 SDCC\n",
      "🔎 SDSC\n",
      "🔎 SEMC\n",
      "🔎 SGBC\n",
      "🔎 SHEC\n",
      "🔎 SIBC\n",
      "🔎 SICC\n",
      "🔎 SIVC\n",
      "🔎 SLBC\n",
      "🔎 SMBC\n",
      "🔎 SNTS\n",
      "🔎 SOGC\n",
      "🔎 SPHC\n",
      "🔎 STAC\n",
      "🔎 STBC\n",
      "🔎 SVOC\n",
      "🔎 TTLC\n",
      "🔎 TTLS\n",
      "🔎 UNLC\n",
      "🔎 UNXC\n",
      "✅ Fichier généré : ratios.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cb81a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📊 ABJC ...\n",
      "📊 BICB ...\n",
      "📊 BICC ...\n",
      "📊 BNBC ...\n",
      "📊 BOAB ...\n",
      "📊 BOABF ...\n",
      "📊 BOAC ...\n",
      "📊 BOAM ...\n",
      "📊 BOAN ...\n",
      "📊 BOAS ...\n",
      "📊 CABC ...\n",
      "📊 CBIBF ...\n",
      "📊 CFAC ...\n",
      "📊 CIEC ...\n",
      "📊 ECOC ...\n",
      "📊 ETIT ...\n",
      "📊 FTSC ...\n",
      "📊 LNBB ...\n",
      "📊 NEIC ...\n",
      "📊 NSBC ...\n",
      "📊 NTLC ...\n",
      "📊 ONTBF ...\n",
      "📊 ORAC ...\n",
      "📊 ORGT ...\n",
      "📊 PALC ...\n",
      "📊 PRSC ...\n",
      "📊 SAFC ...\n",
      "📊 SCRC ...\n",
      "📊 SDCC ...\n",
      "📊 SDSC ...\n",
      "📊 SEMC ...\n",
      "📊 SGBC ...\n",
      "📊 SHEC ...\n",
      "📊 SIBC ...\n",
      "📊 SICC ...\n",
      "📊 SIVC ...\n",
      "📊 SLBC ...\n",
      "📊 SMBC ...\n",
      "📊 SNTS ...\n",
      "📊 SOGC ...\n",
      "📊 SPHC ...\n",
      "📊 STAC ...\n",
      "📊 STBC ...\n",
      "📊 SVOC ...\n",
      "📊 TTLC ...\n",
      "📊 TTLS ...\n",
      "📊 UNLC ...\n",
      "📊 UNXC ...\n",
      "✅ Fichier généré : valorisation_performances.csv\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "440dcd57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📘 ABJC\n",
      "📘 BICB\n",
      "📘 BICC\n",
      "📘 BNBC\n",
      "📘 BOAB\n",
      "📘 BOABF\n",
      "📘 BOAC\n",
      "📘 BOAM\n",
      "📘 BOAN\n",
      "📘 BOAS\n",
      "📘 CABC\n",
      "📘 CBIBF\n",
      "📘 CFAC\n",
      "📘 CIEC\n",
      "📘 ECOC\n",
      "📘 ETIT\n",
      "📘 FTSC\n",
      "📘 LNBB\n",
      "📘 NEIC\n",
      "📘 NSBC\n",
      "📘 NTLC\n",
      "📘 ONTBF\n",
      "📘 ORAC\n",
      "📘 ORGT\n",
      "📘 PALC\n",
      "📘 PRSC\n",
      "📘 SAFC\n",
      "📘 SCRC\n",
      "📘 SDCC\n",
      "📘 SDSC\n",
      "📘 SEMC\n",
      "📘 SGBC\n",
      "📘 SHEC\n",
      "📘 SIBC\n",
      "📘 SICC\n",
      "📘 SIVC\n",
      "📘 SLBC\n",
      "📘 SMBC\n",
      "📘 SNTS\n",
      "📘 SOGC\n",
      "📘 SPHC\n",
      "📘 STAC\n",
      "📘 STBC\n",
      "📘 SVOC\n",
      "📘 TTLC\n",
      "📘 TTLS\n",
      "📘 UNLC\n",
      "📘 UNXC\n",
      "✅ societes_details.csv écrit\n",
      "ℹ️ Aucun fractionnement trouvé\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e5a08db2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import time\n",
    "import pandas as pd\n",
    "from bs4 import BeautifulSoup\n",
    "from playwright.async_api import async_playwright\n",
    "\n",
    "# -------------------- Réglages --------------------\n",
    "BASE = \"https://www.richbourse.com\"\n",
    "LIST_TPL = BASE + \"/common/variation/index/veille/tout?page={}\"  # pagination ?page=1,2,3...\n",
    "UA = (\"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6) AppleWebKit/537.36 \"\n",
    "      \"(KHTML, like Gecko) Chrome/126.0 Safari/537.36\")\n",
    "MAX_PAGES = 500      # garde-fou\n",
    "SLEEP_SEC = 0.25     # petite pause entre pages\n",
    "\n",
    "# -------------------- Utils parsing --------------------\n",
    "def norm(s: str) -> str:\n",
    "    \"\"\"Nettoyage léger du texte extrait du HTML.\"\"\"\n",
    "    return (s or \"\").replace(\"\\xa0\", \" \").replace(\"\\u200b\", \"\").strip()\n",
    "\n",
    "def parse_table_html(html: str) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Retourne un DataFrame à partir du 1er <table> (thead facultatif).\n",
    "    Harmonise automatiquement les en-têtes si absents.\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(html, \"html.parser\")\n",
    "    table = soup.find(\"table\") or soup.find(\"table\", {\"class\": \"table\"})\n",
    "    if table is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Entêtes\n",
    "    headers = []\n",
    "    thead = table.find(\"thead\")\n",
    "    if thead:\n",
    "        headers = [norm(th.get_text(strip=True)) for th in thead.find_all(\"th\")]\n",
    "\n",
    "    # Lignes\n",
    "    tbody = table.find(\"tbody\")\n",
    "    if tbody is None:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    rows = []\n",
    "    for tr in tbody.find_all(\"tr\"):\n",
    "        tds = tr.find_all([\"td\", \"th\"])\n",
    "        if not tds:\n",
    "            continue\n",
    "        cells = [norm(td.get_text(\" \", strip=True)) for td in tds]\n",
    "        if any(cells):\n",
    "            rows.append(cells)\n",
    "\n",
    "    if not rows:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    # Si pas d'en-têtes -> prendre la 1re ligne comme entêtes\n",
    "    if not headers:\n",
    "        headers = [norm(c) for c in rows[0]]\n",
    "        rows = rows[1:]\n",
    "\n",
    "    # Harmoniser largeurs\n",
    "    width = min(len(headers), max((len(r) for r in rows), default=0))\n",
    "    if width == 0:\n",
    "        return pd.DataFrame()\n",
    "    headers = headers[:width]\n",
    "    rows = [r[:width] for r in rows]\n",
    "\n",
    "    try:\n",
    "        df = pd.DataFrame(rows, columns=headers)\n",
    "    except Exception:\n",
    "        # fallback si tailles incohérentes\n",
    "        df = pd.DataFrame(rows)\n",
    "        if len(df.columns) == len(headers):\n",
    "            df.columns = headers\n",
    "    return df\n",
    "\n",
    "async def goto_and_wait(page, url: str):\n",
    "    \"\"\"Navigation + attentes raisonnables pour contenu du tableau.\"\"\"\n",
    "    await page.goto(url, timeout=60000)\n",
    "    await page.wait_for_load_state(\"domcontentloaded\")\n",
    "    try:\n",
    "        await page.wait_for_selector(\"table\", timeout=10000)\n",
    "    except:\n",
    "        pass\n",
    "    try:\n",
    "        await page.wait_for_load_state(\"networkidle\", timeout=5000)\n",
    "    except:\n",
    "        pass\n",
    "\n",
    "# -------------------- Scraper principal --------------------\n",
    "async def scrape_veille_all(headless: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Parcourt toutes les pages du tableau 'Veille' et renvoie un DataFrame consolidé.\n",
    "    \"\"\"\n",
    "    out_frames = []\n",
    "\n",
    "    async with async_playwright() as p:\n",
    "        browser = await p.chromium.launch(\n",
    "            headless=headless,\n",
    "            args=[\"--disable-blink-features=AutomationControlled\"]\n",
    "        )\n",
    "        context = await browser.new_context(user_agent=UA, locale=\"fr-FR\")\n",
    "        page = await context.new_page()\n",
    "\n",
    "        page_no = 1\n",
    "        while page_no <= MAX_PAGES:\n",
    "            url = LIST_TPL.format(page_no)\n",
    "            print(f\"📄 Page {page_no} → {url}\")\n",
    "            await goto_and_wait(page, url)\n",
    "\n",
    "            # petit check de redirection improbable\n",
    "            if \"/user/login\" in page.url or \"/investisseur/profile\" in page.url:\n",
    "                await goto_and_wait(page, url)\n",
    "\n",
    "            try:\n",
    "                html = await page.inner_html(\"table\", timeout=8000)\n",
    "            except:\n",
    "                print(\"  ⚠️ pas de <table> visible — fin.\")\n",
    "                break\n",
    "\n",
    "            df = parse_table_html(html)\n",
    "            if df.empty:\n",
    "                print(\"  ℹ️ aucune ligne — fin de pagination.\")\n",
    "                break\n",
    "\n",
    "            # Ajout d’un marqueur de page (optionnel)\n",
    "            df[\"__page__\"] = page_no\n",
    "            out_frames.append(df)\n",
    "\n",
    "            # Heuristique de fin : très peu de lignes => probablement dernière page\n",
    "            if len(df) < 5:\n",
    "                print(\"  ℹ️ peu de lignes — fin probable.\")\n",
    "                break\n",
    "\n",
    "            page_no += 1\n",
    "            time.sleep(SLEEP_SEC)\n",
    "\n",
    "        await browser.close()\n",
    "\n",
    "    if not out_frames:\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    full = pd.concat(out_frames, ignore_index=True)\n",
    "\n",
    "    # Optionnel : supprimer les colonnes vides intégralement\n",
    "    empty_cols = [c for c in full.columns if full[c].astype(str).str.strip().eq(\"\").all()]\n",
    "    full = full.drop(columns=empty_cols, errors=\"ignore\")\n",
    "\n",
    "    return full\n",
    "\n",
    "# -------------------- Exécution --------------------\n",
    "# Choisissez UNE des deux options ci-dessous :\n",
    "\n",
    "# 1) >>> Dans Jupyter/Notebook :\n",
    "# df = await scrape_veille_all(headless=False)\n",
    "# df.to_csv(\"veille.csv\", index=False)\n",
    "# print(f\"✅ OK : {len(df)} lignes → veille.csv\")\n",
    "# display(df.head())\n",
    "\n",
    "# 2) >>> En script .py (terminal) :\n",
    "# if __name__ == \"__main__\":\n",
    "#     import asyncio\n",
    "#     df = asyncio.run(scrape_veille_all(headless=True))\n",
    "#     df.to_csv(\"veille.csv\", index=False)\n",
    "#     print(f\"✅ OK : {len(df)} lignes → veille.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f094e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/48] ABJC ...\n",
      "[2/48] BICB ...\n",
      "[3/48] BICC ...\n",
      "[4/48] BNBC ...\n",
      "[5/48] BOAB ...\n",
      "[6/48] BOABF ...\n",
      "[7/48] BOAC ...\n",
      "[8/48] BOAM ...\n",
      "[9/48] BOAN ...\n",
      "[10/48] BOAS ...\n",
      "[11/48] CABC ...\n",
      "[12/48] CBIBF ...\n",
      "[13/48] CFAC ...\n",
      "[14/48] CIEC ...\n",
      "[15/48] ECOC ...\n",
      "[16/48] ETIT ...\n",
      "[17/48] FTSC ...\n",
      "[18/48] LNBB ...\n",
      "[19/48] NEIC ...\n",
      "[20/48] NSBC ...\n",
      "[21/48] NTLC ...\n",
      "[22/48] ONTBF ...\n",
      "[23/48] ORAC ...\n",
      "[24/48] ORGT ...\n",
      "[25/48] PALC ...\n",
      "[26/48] PRSC ...\n",
      "[27/48] SAFC ...\n",
      "[28/48] SCRC ...\n",
      "[29/48] SDCC ...\n",
      "[30/48] SDSC ...\n",
      "[31/48] SEMC ...\n",
      "[32/48] SGBC ...\n",
      "[33/48] SHEC ...\n",
      "[34/48] SIBC ...\n",
      "[35/48] SICC ...\n",
      "[36/48] SIVC ...\n",
      "[37/48] SLBC ...\n",
      "[38/48] SMBC ...\n",
      "[39/48] SNTS ...\n",
      "[40/48] SOGC ...\n",
      "[41/48] SPHC ...\n",
      "[42/48] STAC ...\n",
      "[43/48] STBC ...\n",
      "[44/48] SVOC ...\n",
      "[45/48] TTLC ...\n",
      "[46/48] TTLS ...\n",
      "[47/48] UNLC ...\n",
      "[48/48] UNXC ...\n",
      "\n",
      "Terminé.\n",
      "CSV : richbourse_societes.csv\n",
      "XLSX : richbourse_societes.xlsx\n",
      "Logos : /Users/jeanjoelgoli/Documents/FINANCE/Travaux BRVM/logos_richbourse\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2537159d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1/48] ABJC …\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/0n/svq7n_mn2lg1fv5jbk66hzg00000gn/T/ipykernel_12105/1316626944.py:176: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
      "  \"scraped_at\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2/48] BICB …\n",
      "[3/48] BICC …\n",
      "[4/48] BNBC …\n",
      "[5/48] BOAB …\n",
      "[6/48] BOABF …\n",
      "[7/48] BOAC …\n",
      "[8/48] BOAM …\n",
      "[9/48] BOAN …\n",
      "[10/48] BOAS …\n",
      "[11/48] CABC …\n",
      "[12/48] CBIBF …\n",
      "[13/48] CFAC …\n",
      "[14/48] CIEC …\n",
      "[15/48] ECOC …\n",
      "[16/48] ETIT …\n",
      "[17/48] FTSC …\n",
      "[18/48] LNBB …\n",
      "[19/48] NEIC …\n",
      "[20/48] NSBC …\n",
      "[21/48] NTLC …\n",
      "[22/48] ONTBF …\n",
      "[23/48] ORAC …\n",
      "[24/48] ORGT …\n",
      "[25/48] PALC …\n",
      "[26/48] PRSC …\n",
      "[27/48] SAFC …\n",
      "[28/48] SCRC …\n",
      "[29/48] SDCC …\n",
      "[30/48] SDSC …\n",
      "[31/48] SEMC …\n",
      "[32/48] SGBC …\n",
      "[33/48] SHEC …\n",
      "[34/48] SIBC …\n",
      "[35/48] SICC …\n",
      "[36/48] SIVC …\n",
      "[37/48] SLBC …\n",
      "[38/48] SMBC …\n",
      "[39/48] SNTS …\n",
      "[40/48] SOGC …\n",
      "[41/48] SPHC …\n",
      "[42/48] STAC …\n",
      "[43/48] STBC …\n",
      "[44/48] SVOC …\n",
      "[45/48] TTLC …\n",
      "[46/48] TTLS …\n",
      "[47/48] UNLC …\n",
      "[48/48] UNXC …\n",
      "\n",
      "OK. CSV: richbourse_mouvements.csv | XLSX: richbourse_mouvements.xlsx\n"
     ]
    }
   ],
   "source": [
    "#!/usr/bin/env python3\n",
    "# -*- coding: utf-8 -*-\n",
    "\n",
    "import re\n",
    "import time\n",
    "from datetime import datetime\n",
    "from typing import Dict, Optional, List\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "TICKERS = [\n",
    "    \"ABJC\",\"BICB\",\"BICC\",\"BNBC\",\"BOAB\",\"BOABF\",\"BOAC\",\"BOAM\",\"BOAN\",\"BOAS\",\n",
    "    \"CABC\",\"CBIBF\",\"CFAC\",\"CIEC\",\"ECOC\",\"ETIT\",\"FTSC\",\"LNBB\",\"NEIC\",\"NSBC\",\n",
    "    \"NTLC\",\"ONTBF\",\"ORAC\",\"ORGT\",\"PALC\",\"PRSC\",\"SAFC\",\"SCRC\",\"SDCC\",\"SDSC\",\n",
    "    \"SEMC\",\"SGBC\",\"SHEC\",\"SIBC\",\"SICC\",\"SIVC\",\"SLBC\",\"SMBC\",\"SNTS\",\"SOGC\",\n",
    "    \"SPHC\",\"STAC\",\"STBC\",\"SVOC\",\"TTLC\",\"TTLS\",\"UNLC\",\"UNXC\"\n",
    "]\n",
    "\n",
    "BASE_URL = \"https://www.richbourse.com/common/mouvements/index/{ticker}\"\n",
    "OUT_CSV  = \"richbourse_mouvements.csv\"\n",
    "OUT_XLSX = \"richbourse_mouvements.xlsx\"\n",
    "\n",
    "# Mapping canonique -> variantes possibles qu'on peut voir à gauche\n",
    "FIELD_MAP = {\n",
    "    \"Prix (FCFA)\": [r\"^\\s*\\d[\\d\\s.,]*\\s*FCFA\\s*$\"],  # gros prix en haut (on le traite à part)\n",
    "    \"Variation (%)\": [r\"%\"],                         # idem (on le traite à part)\n",
    "    \"Volume (titres)\": [r\"^Volume\\s*\\(titres\\)\\s*$\"],\n",
    "    \"Valeur (FCFA)\": [r\"^Valeur\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Ouverture (FCFA)\": [r\"^Ouverture\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Plus haut (FCFA)\": [r\"^Plus haut\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Plus bas (FCFA)\": [r\"^Plus bas\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Clôture jour (FCFA)\": [r\"^Clôture jour\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Clôture veille (FCFA)\": [r\"^Clôture veille\\s*\\(FCFA\\)\\s*$\"],\n",
    "    \"Nombre total de titres\": [r\"^Nombre total de titres\\s*$\"],\n",
    "    \"Titres du flottant\": [r\"^Titres du flottant\\s*$\"],\n",
    "    \"Volume moyen\": [r\"^Volume moyen\\s*$\"],\n",
    "    \"Valorisation (MFCFA)\": [r\"^Valorisation\\s*\\(MFCFA\\)\\s*$\"],\n",
    "}\n",
    "\n",
    "CANON_KEYS = list(FIELD_MAP.keys())\n",
    "\n",
    "def make_session() -> requests.Session:\n",
    "    s = requests.Session()\n",
    "    s.headers.update({\n",
    "        \"User-Agent\": (\n",
    "            \"Mozilla/5.0 (Macintosh; Intel Mac OS X 14_6) \"\n",
    "            \"AppleWebKit/537.36 (KHTML, like Gecko) \"\n",
    "            \"Chrome/126.0 Safari/537.36\"\n",
    "        ),\n",
    "        \"Accept-Language\": \"fr-FR,fr;q=0.9\",\n",
    "    })\n",
    "    s.mount(\"https://\", requests.adapters.HTTPAdapter(max_retries=3))\n",
    "    s.mount(\"http://\", requests.adapters.HTTPAdapter(max_retries=3))\n",
    "    return s\n",
    "\n",
    "SESSION = make_session()\n",
    "\n",
    "num_re = re.compile(r\"[-+]?\\d[\\d\\s.,]*\")\n",
    "\n",
    "def to_float(s: Optional[str]) -> Optional[float]:\n",
    "    if not s:\n",
    "        return None\n",
    "    s = str(s)\n",
    "    m = num_re.search(s)\n",
    "    if not m:\n",
    "        return None\n",
    "    n = m.group(0).replace(\" \", \"\").replace(\"\\xa0\", \"\").replace(\",\", \".\")\n",
    "    try:\n",
    "        return float(n)\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def to_int(s: Optional[str]) -> Optional[int]:\n",
    "    f = to_float(s)\n",
    "    return int(round(f)) if f is not None else None\n",
    "\n",
    "def guess_left_panel(soup: BeautifulSoup):\n",
    "    \"\"\"\n",
    "    Essaie de cibler la colonne/panneau de gauche.\n",
    "    Heuristique: bloc qui contient les libellés 'Volume (titres)', 'Ouverture (FCFA)', etc.\n",
    "    \"\"\"\n",
    "    candidates = soup.find_all(True, recursive=True)\n",
    "    best = None\n",
    "    score_best = -1\n",
    "    wanted_labels = [\"Volume (titres)\", \"Ouverture (FCFA)\", \"Clôture veille (FCFA)\"]\n",
    "    for tag in candidates:\n",
    "        txt = \" \".join(tag.stripped_strings)\n",
    "        score = sum(1 for w in wanted_labels if w in txt)\n",
    "        if score > score_best:\n",
    "            score_best = score\n",
    "            best = tag\n",
    "    return best or soup\n",
    "\n",
    "def extract_price_and_change(soup_left) -> (Optional[float], Optional[float]):\n",
    "    \"\"\"\n",
    "    Récupère le gros prix en FCFA et la variation en % dans le bandeau vert (ou voisin).\n",
    "    Heuristique: chercher une ligne avec 'FCFA' puis à proximité '%'.\n",
    "    \"\"\"\n",
    "    text = \" \".join(soup_left.stripped_strings)\n",
    "    # prix FCFA (le plus grand premier match)\n",
    "    price = None\n",
    "    for m in re.finditer(r\"\\d[\\d\\s.,]*\\s*FCFA\", text):\n",
    "        price_str = m.group(0)\n",
    "        price = to_float(price_str)\n",
    "        if price is not None:\n",
    "            break\n",
    "    # variation %\n",
    "    change = None\n",
    "    m = re.search(r\"[-+]?\\d[\\d\\s.,]*\\s*%\", text)\n",
    "    if m:\n",
    "        change = to_float(m.group(0))\n",
    "    return price, change\n",
    "\n",
    "def extract_kv_pairs(soup_left) -> Dict[str, str]:\n",
    "    \"\"\"\n",
    "    Parcourt le bloc pour trouver des paires Libellé -> Valeur.\n",
    "    Beaucoup de pages utilisent des <tr><td> ou des <div> en deux colonnes.\n",
    "    On détecte les libellés connus via FIELD_MAP, puis on prend la valeur voisine.\n",
    "    \"\"\"\n",
    "    results: Dict[str, str] = {}\n",
    "    # Stratégie 1: chercher des TR / TD\n",
    "    for tr in soup_left.find_all(\"tr\"):\n",
    "        tds = [normalize(\" \".join(td.stripped_strings)) for td in tr.find_all(\"td\")]\n",
    "        if len(tds) >= 2:\n",
    "            label, value = tds[0], tds[1]\n",
    "            canon = canonize_label(label)\n",
    "            if canon:\n",
    "                results[canon] = value\n",
    "    # Stratégie 2: blocs <div> \"ligne\"\n",
    "    # On ne remplace pas ce qui a déjà été trouvé\n",
    "    divs = soup_left.find_all(\"div\")\n",
    "    for i, div in enumerate(divs):\n",
    "        label = normalize(div.get_text(\" \", strip=True))\n",
    "        canon = canonize_label(label)\n",
    "        if canon and canon not in results:\n",
    "            # valeur: essayer frère suivant, ou texte du parent immédiat\n",
    "            value = None\n",
    "            sib = div.find_next_sibling()\n",
    "            if sib:\n",
    "                value = normalize(sib.get_text(\" \", strip=True))\n",
    "            if not value:\n",
    "                parent = div.parent\n",
    "                if parent:\n",
    "                    txts = [normalize(x) for x in parent.stripped_strings]\n",
    "                    if len(txts) >= 2 and normalize(txts[0]) == label:\n",
    "                        value = txts[1]\n",
    "            if value:\n",
    "                results[canon] = value\n",
    "    # Stratégie 3: lecture linéaire par proximité (fallback)\n",
    "    if len(results) < 5:\n",
    "        items = [normalize(s) for s in soup_left.stripped_strings]\n",
    "        for j, token in enumerate(items[:-1]):\n",
    "            canon = canonize_label(token)\n",
    "            if canon and canon not in results:\n",
    "                results[canon] = items[j+1]\n",
    "    return results\n",
    "\n",
    "def normalize(s: str) -> str:\n",
    "    return re.sub(r\"\\s+\", \" \", s or \"\").strip()\n",
    "\n",
    "def canonize_label(label: str) -> Optional[str]:\n",
    "    lab = normalize(label)\n",
    "    for canon, patterns in FIELD_MAP.items():\n",
    "        for pat in patterns:\n",
    "            if re.search(pat, lab, flags=re.IGNORECASE):\n",
    "                return canon\n",
    "    return None\n",
    "\n",
    "def scrape_one(ticker: str) -> Dict[str, Optional[str]]:\n",
    "    url = BASE_URL.format(ticker=ticker)\n",
    "    row: Dict[str, Optional[str]] = {\n",
    "        \"ticker\": ticker,\n",
    "        \"source_url\": url,\n",
    "        \"scraped_at\": datetime.utcnow().isoformat(timespec=\"seconds\") + \"Z\",\n",
    "        # valeurs brutes\n",
    "        \"Prix (FCFA)\": None,\n",
    "        \"Variation (%)\": None,\n",
    "        \"Volume (titres)\": None,\n",
    "        \"Valeur (FCFA)\": None,\n",
    "        \"Ouverture (FCFA)\": None,\n",
    "        \"Plus haut (FCFA)\": None,\n",
    "        \"Plus bas (FCFA)\": None,\n",
    "        \"Clôture jour (FCFA)\": None,\n",
    "        \"Clôture veille (FCFA)\": None,\n",
    "        \"Nombre total de titres\": None,\n",
    "        \"Titres du flottant\": None,\n",
    "        \"Volume moyen\": None,\n",
    "        \"Valorisation (MFCFA)\": None,\n",
    "        # versions numériques utiles\n",
    "        \"prix_fcfa\": None,\n",
    "        \"variation_pct\": None,\n",
    "        \"volume_titres\": None,\n",
    "        \"valeur_fcfa\": None,\n",
    "        \"ouverture_fcfa\": None,\n",
    "        \"plus_haut_fcfa\": None,\n",
    "        \"plus_bas_fcfa\": None,\n",
    "        \"cloture_jour_fcfa\": None,\n",
    "        \"cloture_veille_fcfa\": None,\n",
    "        \"nb_total_titres\": None,\n",
    "        \"titres_flottant\": None,\n",
    "        \"volume_moyen\": None,\n",
    "        \"valorisation_mfcfa\": None,\n",
    "        \"valorisation_fcfa\": None,\n",
    "    }\n",
    "    try:\n",
    "        r = SESSION.get(url, timeout=25)\n",
    "        r.raise_for_status()\n",
    "        soup = BeautifulSoup(r.text, \"lxml\")\n",
    "        left = guess_left_panel(soup)\n",
    "\n",
    "        # prix + variation\n",
    "        price, change = extract_price_and_change(left)\n",
    "        if price is not None:\n",
    "            row[\"Prix (FCFA)\"] = f\"{price:.0f} FCFA\"\n",
    "            row[\"prix_fcfa\"] = price\n",
    "        if change is not None:\n",
    "            row[\"Variation (%)\"] = f\"{change:.2f}%\"\n",
    "            row[\"variation_pct\"] = change\n",
    "\n",
    "        # paires label/valeur\n",
    "        kv = extract_kv_pairs(left)\n",
    "        for k in CANON_KEYS:\n",
    "            if k in kv:\n",
    "                row[k] = kv[k]\n",
    "\n",
    "        # cast numériques\n",
    "        row[\"volume_titres\"] = to_int(row[\"Volume (titres)\"])\n",
    "        row[\"valeur_fcfa\"] = to_float(row[\"Valeur (FCFA)\"])\n",
    "        row[\"ouverture_fcfa\"] = to_float(row[\"Ouverture (FCFA)\"])\n",
    "        row[\"plus_haut_fcfa\"] = to_float(row[\"Plus haut (FCFA)\"])\n",
    "        row[\"plus_bas_fcfa\"] = to_float(row[\"Plus bas (FCFA)\"])\n",
    "        row[\"cloture_jour_fcfa\"] = to_float(row[\"Clôture jour (FCFA)\"])\n",
    "        row[\"cloture_veille_fcfa\"] = to_float(row[\"Clôture veille (FCFA)\"])\n",
    "        row[\"nb_total_titres\"] = to_int(row[\"Nombre total de titres\"])\n",
    "        row[\"titres_flottant\"] = to_int(row[\"Titres du flottant\"])\n",
    "        row[\"volume_moyen\"] = to_int(row[\"Volume moyen\"])\n",
    "        row[\"valorisation_mfcfa\"] = to_float(row[\"Valorisation (MFCFA)\"])\n",
    "        if row[\"valorisation_mfcfa\"] is not None:\n",
    "            row[\"valorisation_fcfa\"] = row[\"valorisation_mfcfa\"] * 1_000_000.0\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"[{ticker}] ERREUR: {e}\")\n",
    "\n",
    "    return row\n",
    "\n",
    "def scrape_all(tickers: List[str]) -> pd.DataFrame:\n",
    "    rows = []\n",
    "    for i, t in enumerate(tickers, 1):\n",
    "        print(f\"[{i}/{len(tickers)}] {t} …\")\n",
    "        rows.append(scrape_one(t))\n",
    "        time.sleep(0.6)  # politesse\n",
    "    cols_order = [\n",
    "        \"ticker\",\"source_url\",\"scraped_at\",\n",
    "        \"Prix (FCFA)\",\"Variation (%)\",\n",
    "        \"Volume (titres)\",\"Valeur (FCFA)\",\"Ouverture (FCFA)\",\n",
    "        \"Plus haut (FCFA)\",\"Plus bas (FCFA)\",\n",
    "        \"Clôture jour (FCFA)\",\"Clôture veille (FCFA)\",\n",
    "        \"Nombre total de titres\",\"Titres du flottant\",\n",
    "        \"Volume moyen\",\"Valorisation (MFCFA)\",\n",
    "        \"prix_fcfa\",\"variation_pct\",\"volume_titres\",\"valeur_fcfa\",\n",
    "        \"ouverture_fcfa\",\"plus_haut_fcfa\",\"plus_bas_fcfa\",\n",
    "        \"cloture_jour_fcfa\",\"cloture_veille_fcfa\",\n",
    "        \"nb_total_titres\",\"titres_flottant\",\"volume_moyen\",\n",
    "        \"valorisation_mfcfa\",\"valorisation_fcfa\"\n",
    "    ]\n",
    "    df = pd.DataFrame(rows)\n",
    "    return df[cols_order]\n",
    "\n",
    "def main():\n",
    "    df = scrape_all(TICKERS)\n",
    "    df.to_csv(OUT_CSV, index=False, encoding=\"utf-8\")\n",
    "    try:\n",
    "        df.to_excel(OUT_XLSX, index=False)\n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Impossible d’écrire l’XLSX (openpyxl manquant ?): {e}\")\n",
    "    print(f\"\\nOK. CSV: {OUT_CSV} | XLSX: {OUT_XLSX}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
